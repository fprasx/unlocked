<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Building a Rusty, Lock-free Dynamically Resizable Array</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro/intro.html">Introduction</a></li><li class="chapter-item expanded "><a href="intro/goal.html"><strong aria-hidden="true">1.</strong> This Book</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="intro/message.html"><strong aria-hidden="true">1.1.</strong> A Message</a></li></ol></li><li class="chapter-item expanded "><a href="concurrency/intro.html"><strong aria-hidden="true">2.</strong> Concurrency</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="concurrency/keywords.html"><strong aria-hidden="true">2.1.</strong> Keywords</a></li><li class="chapter-item expanded "><a href="concurrency/concepts.html"><strong aria-hidden="true">2.2.</strong> Concepts</a></li></ol></li><li class="chapter-item expanded "><a href="atomics/intro.html"><strong aria-hidden="true">3.</strong> Atomics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="atomics/memory_orderings.html"><strong aria-hidden="true">3.1.</strong> What are Memory Orderings?</a></li><li class="chapter-item expanded "><a href="atomics/cas.html"><strong aria-hidden="true">3.2.</strong> Compare-and-Swap</a></li></ol></li><li class="chapter-item expanded "><a href="paper/intro.html"><strong aria-hidden="true">4.</strong> Introduction to the Paper</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="paper/structure_memory.html"><strong aria-hidden="true">4.1.</strong> Structure: Memory</a></li><li class="chapter-item expanded "><a href="paper/structure_sync.html"><strong aria-hidden="true">4.2.</strong> Structure: Synchronization</a></li><li class="chapter-item expanded "><a href="paper/algorithm.html"><strong aria-hidden="true">4.3.</strong> The Algorithm</a></li></ol></li><li class="chapter-item expanded "><a href="code/starting_code.html"><strong aria-hidden="true">5.</strong> Starting Code</a></li><li class="chapter-item expanded "><a href="code/structs.html"><strong aria-hidden="true">6.</strong> Memory Allocation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="code/alloc/get.html"><strong aria-hidden="true">6.1.</strong> get</a></li><li class="chapter-item expanded "><a href="code/alloc/allocate_bucket.html"><strong aria-hidden="true">6.2.</strong> allocate_bucket</a></li><li class="chapter-item expanded "><a href="code/alloc/reserve.html"><strong aria-hidden="true">6.3.</strong> reserve</a></li></ol></li><li class="chapter-item expanded "><a href="code/ops/ops.html"><strong aria-hidden="true">7.</strong> Operations</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="code/ops/new.html"><strong aria-hidden="true">7.1.</strong> new</a></li><li class="chapter-item expanded "><a href="code/ops/complete_write.html"><strong aria-hidden="true">7.2.</strong> complete_write</a></li><li class="chapter-item expanded "><a href="code/ops/push.html"><strong aria-hidden="true">7.3.</strong> push</a></li><li class="chapter-item expanded "><a href="code/ops/pop.html"><strong aria-hidden="true">7.4.</strong> pop</a></li><li class="chapter-item expanded "><a href="code/ops/size.html"><strong aria-hidden="true">7.5.</strong> size</a></li><li class="chapter-item expanded "><a href="code/ops/tests.html"><strong aria-hidden="true">7.6.</strong> tests</a></li></ol></li><li class="chapter-item expanded "><a href="code/reclaim/problem.html"><strong aria-hidden="true">8.</strong> Memory Reclamation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="code/reclaim/hazptr.html"><strong aria-hidden="true">8.1.</strong> Hazard Pointers</a></li><li class="chapter-item expanded "><a href="code/reclaim/complete_write.html"><strong aria-hidden="true">8.2.</strong> Fixing complete_write</a></li><li class="chapter-item expanded "><a href="code/reclaim/push_pop.html"><strong aria-hidden="true">8.3.</strong> Fixing push &amp; pop</a></li><li class="chapter-item expanded "><a href="code/reclaim/vector_drop.html"><strong aria-hidden="true">8.4.</strong> Dropping the vector</a></li><li class="chapter-item expanded "><a href="code/reclaim/tests.html"><strong aria-hidden="true">8.5.</strong> More tests</a></li></ol></li><li class="chapter-item expanded "><a href="reflections/reflection.html"><strong aria-hidden="true">9.</strong> Reflections</a></li><li class="chapter-item expanded "><a href="closing/acknowledgements.html"><strong aria-hidden="true">10.</strong> Acknowledgements</a></li><li class="chapter-item expanded "><a href="closing/resources.html"><strong aria-hidden="true">11.</strong> Helpful Resources</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Building a Rusty, Lock-free Dynamically Resizable Array</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="building-a-rusty-lock-free-dynamically-resizable-array"><a class="header" href="#building-a-rusty-lock-free-dynamically-resizable-array">Building a Rusty, Lock-free Dynamically Resizable Array</a></h1>
<p>Dedicated to HS, whom I wouldn't compare-and-swap with anyone else.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-goal"><a class="header" href="#the-goal">The Goal</a></h1>
<p>This book has a few goals.</p>
<p>Inspired by
<a href="https://rust-unofficial.github.io/too-many-lists/">Learn Rust With Entirely Too Many Linked Lists</a>,
the main goal of this book is too teach you some Rust while implementing a
useful container (although the usefulness of linked lists is debatable üòâ).
We'll be implementing the Lock-free vector described in the paper
<a href="https://www.stroustrup.com/lock-free-vector.pdf">Lock-free Dynamically Resizable Arrays</a>
by <strong>Dechev et al., 2006</strong></p>
<p>I hope that this book will inspire other new Rustaceans like myself to push
their capabilities. I also hope that non-Rustaceans will see the how awesome
Rust is as well. No matter whether you code or not, I hope that this book will
show you a interesting area of computer science and a beautiful language!</p>
<h2 id="topics-well-cover"><a class="header" href="#topics-well-cover">Topics We'll Cover</a></h2>
<ul>
<li>Concurrency
<ul>
<li>Lock-full (My own term)</li>
<li>Lock-free</li>
</ul>
</li>
<li>Atomics
<ul>
<li>Memory Orderings</li>
<li>Compare-and-Swap</li>
</ul>
</li>
<li>Memory Management
<ul>
<li>Allocations in Rust</li>
<li>Hazard pointers</li>
</ul>
</li>
<li>Using Unsafe Rust
<ul>
<li>Raw Pointers</li>
</ul>
</li>
<li><strong>Anything else I find interesting!</strong></li>
</ul>
<h2 id="necessary-experience"><a class="header" href="#necessary-experience">Necessary Experience</a></h2>
<h3 id="tldr-its-good-to-know-some-rust"><a class="header" href="#tldr-its-good-to-know-some-rust">tl;dr it's good to know some Rust</a></h3>
<p>It will be helpful to be familiar with Rust or another language like C and C++,
as we will be dealing with low-level constructs like pointers, atomics, and
memory management. <strong>However</strong>, even if you are only familiar with <code>Some(_)</code> or
<code>None</code> of these things, I believe you will be able to learn an interesting thing
or two.</p>
<p>Of course, the code will be in Rust, so prior knowledge will be helpful. I'm not
going to spend time explaining syntax. However, I will comment the code well and
explain what is going on. I think if you're comfortable with the first 15
chapters of <a href="https://doc.rust-lang.org/book/">The Book</a>, you should be fine.
Even if not, as long as you understand most of Rust syntax and are fine with
looking something up everyonce in a while, you'll be fine.
<a href="https://doc.rust-lang.org/book/ch16-00-concurrency.html">Chapter 16</a> is very
helpful though as it's the chapter on concurrency.</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="a-message-from-me"><a class="header" href="#a-message-from-me">A Message From Me</a></h2>
<p>Although I want you to come away from this book having learned something, and
hopefully feeling inspired to write more exciting code, this book is also a
learning experience for me.</p>
<p>I've never used Markdown before except for writing GitHub README's that only I'm
going to read, so simply using Mdbook is super exciting! I think Markdown is
cool because it's like HTML for lazy people (and I am <strong>very</strong> lazy).</p>
<p>Throughout the book, I will have many questions. I started teaching myself Rust
around ~half a year ago, so perhaps I'm not really qualified to write this book.
I'd never touched an atomic variable before I started this project (and I still
<em>technically</em> haven't), so I really mean it when I say this is a learning
experience for me. I'll try to document the answers to the questions I have so
you can learn from them as well. There will also be a whole section of
reflections, as this is also for a school project. You might enjoy that section
more than the technical sections.</p>
<p>And without further ado . . . <code>cargo run!</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concurrency"><a class="header" href="#concurrency">Concurrency</a></h1>
<p><strong>Concurrent</strong> (Merriam-Webster): operating or occuring at the same time</p>
<p>Concurrent programming is simply programming that involves more than one events
happening at a time, in the sense that we think of events in a program
happening. In a non-concurrent program, if we wanted to print out two numbers,
we would print out one, and <em>then</em> the other. In a concurrent approach, we might
spawn two threads, and assign each of them a number to print. A big idea in
concurrent programming is having multiple processes running at the same time.
You can think of it like your computer running Firefox <em>and</em> Spotify at the same
time.<sup class="footnote-reference"><a href="#1">1</a></sup></p>
<p>On a hardware level, concurrency is possible because CPU's have multiple cores
(processors, the chips that do the math). Thus, we can add two numbers on one
core while dividing two numbers on another core.</p>
<p><sup class="footnote-reference"><a href="#1">1</a></sup> Your computer might actually just be switching between the applications
really fast if you only have one CPU core. Even if you have many cores, it's
possible that the the applications could be running on the same core. It's all
up to the task scheduler.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="keywords"><a class="header" href="#keywords">Keywords</a></h1>
<ul>
<li>Shared-state</li>
<li>Lock-full</li>
<li>Lock-free</li>
<li>buffer</li>
<li>cache</li>
<li>the rust alloc crate</li>
<li>null pointer</li>
<li>heap</li>
<li>stack (see the rust book)</li>
</ul>
<h1 id="thread"><a class="header" href="#thread">Thread</a></h1>
<h1 id="mutex"><a class="header" href="#mutex">Mutex</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="concepts"><a class="header" href="#concepts">Concepts</a></h1>
<ul>
<li>Processes (<a href="concurrency/./keywords.html">Threads</a>)</li>
<li>Data Race (Use google drive example)</li>
<li><a href="concurrency/./keywords.html#mutex">Mutual Exclusion</a></li>
<li>Readers and Writers</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="atomics"><a class="header" href="#atomics">Atomics</a></h1>
<p>Besides having a cool name, atomics are crucial for writing concurrent code.</p>
<p>We first need to think about how computers perform operations. On a a 32-bit
machine, loading (reading) a 64-bit value would require two CPU operations, one
for the first 32 bits and one for the second 32 bits.</p>
<p>Suppose each box represents a byte (8 bits):</p>
<pre><code>
ÀÖ    Load 1             ÀÖ
+-----+-----+-----+-----+-----+-----+-----+-----+
|     |     |     |     |     |     |     |     |
+-----+-----+-----+-----+-----+-----+-----+-----+
                        ^         Load 2        ^
</code></pre>
<p>This shows how a load of a variable can take multiple steps.</p>
<p><strong>Atomic</strong> operations take only one step. They have no intermediate observable
state, which means the CPU only observes them as having happened or not.</p>
<p>This is very important in multithreaded scenarios because if threads use
non-atomic operations, loads and scores might end up overlapping, resulting in
<em>torn</em> reads and writes.</p>
<p>For example, on our hypothetical 32-bit machine, one core might finish the first
write to the 32-bit value, another core then might before the two loads needed
to load the value, and then the first core might finish the storing the last 32
bits. Now, one core has a value that is half gibberish!</p>
<p>This is an example of a data race, and could result in undefined behavior.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="what-are-memory-orderings"><a class="header" href="#what-are-memory-orderings">What are Memory Orderings?</a></h1>
<p>In a concurrent environment, each variable has a modification history, all the
values it has been. Say we have a variable A. We could store 1 into it, then 2,
then 3.</p>
<p>The problem comes from the fact that another thread reading A can &quot;read&quot; any of
those values, <strong>even after the last store is executed, &quot;in real time&quot;.</strong> For
example, it might have an older (stale) copy of the variable cached.</p>
<p>To ensure that our programs run the way we want, we need to specify more
explicitly which values in the modification history the CPU is allowed to use.</p>
<p>Another problem is the CPU reordering instructions. The Golden Rule of
instruction reordering is that reordering cannot change the effects of the
program from the perspective of the thread. That is, the running thread, <em>not</em>
other threads. The CPU might not think it's doing anything wrong moving some
instructions around. And from the perspective of the thread it's executing,
everything will seem alright. Other threads might be receiving crazy results
though.</p>
<p>An <em>Ordering</em> is a parameter you provide to operations with atomic variables
that specifies which reorderings can happen and which values in the modification
history the CPU can use.</p>
<p>I'm not going to go super in-depth into the intricacies of each ordering, but I
will explain the important parts of each. If you're curious, Jon Gjenset has a
great youtube video on Atomics, which sections on each ordering:
<a href="https://www.youtube.com/watch?v=rMGWeSjctlY">Crust of Rust: Atomics and Memory Ordering</a></p>
<h2 id="relaxed"><a class="header" href="#relaxed">Relaxed</a></h2>
<p>The first ordering is <code>Relaxed</code>. This provides no guarantees on Ordering, simply
that loads/stores are concurrent. The classic use case (I think this use case is
classic at least, I always see it used in examples) of the <code>Relaxed</code> ordering is
incrementing/decrementing a counter. We don't really care about observing the of
the counter; we just want to make sure our updates happen. When we finally load
the counter, we can use an ordering with stronger guarantees.</p>
<h2 id="release"><a class="header" href="#release">Release</a></h2>
<p>This ordering is used with stores. You can think of <code>Release</code> as <code>Release</code>ing a
lock. We want any changes that happened while we had the lock to become visible
to other threads. When you store with <code>Release</code>, it's like saying &quot;I'm done with
this, use these changes.&quot; More specifically, when you store with release, all
changes to the variable are ordered before any <code>Acquire</code> loads.</p>
<pre><code>STORE (Relaxed) ‚îÄ‚îê
STORE (Release) -+-// &quot;Release the lock&quot;
LOAD (Acquire)   ‚îÇ
    X          &lt;‚îÄ‚îò // nope, can't reorder Release store after Acquire load
</code></pre>
<p>The compiler can't reorder the <code>Relaxed</code> store after the <code>Release</code> store,
guaranteeing that other threads see both stores.</p>
<h2 id="acquire"><a class="header" href="#acquire">Acquire</a></h2>
<p>This is used with loads. You can think of <code>Acquire</code> like <code>Acquire</code>ing a lock.
This means that no operations should get reordered <em>before</em> taking the lock.
When you load with <code>Acquire</code>, no reads or writes get reordered before that load.
Anything that happens after &quot;taking the lock&quot; stays after the &quot;lock was taken&quot;</p>
<pre><code>    X                              &lt;‚îÄ‚îê // nope, can't reorder store before Acquire load
    X               &lt;‚îÄ‚îê              ‚îÇ // nope, can't reorder load before Acquire load
LOAD (Acquire) -------+--------------+-// &quot;Take the lock&quot;
STORE (Relaxed)      ‚îÄ‚îò              ‚îÇ
LOAD a different variable (Relaxed) ‚îÄ‚îò
</code></pre>
<p>Anything we do while &quot;holding the lock&quot;, cannot get reordered before &quot;taking the
lock&quot;.</p>
<p>Note: Although the lock metaphor is helpful for understanding <code>Acquire</code> and
<code>Release</code>, remember there are no actual locks involved.</p>
<blockquote>
<p>How is synchronization achieved? You see, when two <code>Ordering</code>s love each other
very much . . . we get <code>Acquire-Release</code> semantics. Watch what happens when we
use <code>Acquire</code> and <code>Release</code> together (diagram inspired by
<a href="https://preshing.com/20120913/acquire-and-release-semantics/">this blog post</a>):</p>
<!-- prettier-ignore-start -->
<pre><code>‚îî‚îÄ‚îÄ‚îÄ‚îò Release store

  | Read most recent data because the load is Acquire and the store is Release
  V

‚îå‚îÄ‚îÄ‚îÄ‚îê Acquire load
Memory operations cannot go above


Memory operations cannot go below
‚îî‚îÄ‚îÄ‚îÄ‚îò Release store

  | Read most recent data because the load is Acquire and the store is Release
  V

‚îå‚îÄ‚îÄ‚îÄ‚îê Acquire load
</code></pre>
<!-- prettier-ignore-end -->
<p>All operations are trapped in their own sections, and each section gets the
most recent modifications because of the way <code>Acquire</code> loads and <code>Release</code>
stores synchronize!</p>
</blockquote>
<h2 id="acqrel-acquire-and-release"><a class="header" href="#acqrel-acquire-and-release">AcqRel (Acquire <em>and</em> Release)</a></h2>
<p>An <code>AcqRel</code> load/store is just <code>Release</code> for stores and <code>Acquire</code> for loads. I
haven't really been able to discern when this ordering is used. I think one use
case is Read-Modify-Write operations, like loading a variable, multiplying it by
two, and storing it back. We want to the load to be <code>Acquire</code> and the store
<code>Release</code> so we would use <code>AcqRel</code> to achieve this. Even still, I have rarely
seen this Ordering used.</p>
<h2 id="seqcst-sequentially-consistent"><a class="header" href="#seqcst-sequentially-consistent">SeqCst (Sequentially Consistent)</a></h2>
<p>The <code>SeqCst</code> ordering makes has the same reordering effects of <code>AcqRel</code>, and
also establishes a consistent modification order across all threads. Two stores
tagged <code>Relaxed</code> might show up in different orders to different threads.
However, if they are both tagged <code>SeqCst</code>, they will show up in the same order
to all threads. <code>SeqCst</code> is the strongest ordering, and thus also the safest
(see Jon Gjenset's video for weird things that can happen with weaker
orderings). Safety comes at a price though, with the compiler often having to
emit <em>memory fences</em><sup class="footnote-reference"><a href="#1">1</a></sup> to guarantee sequential consistency. This can affect
performance.</p>
<p><sup class="footnote-reference"><a href="#1">1</a></sup> A memory fence prevents the CPU from reordering operations in certain ways.
This is a great
<a href="https://preshing.com/20120710/memory-barriers-are-like-source-control-operations/">article</a>
which describes many different types of fences, kind of like the different
Atomic orderings, which restrict the compiler instead of the CPU.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="compare-and-swap-also-known-as-cas-and-compare-exchange"><a class="header" href="#compare-and-swap-also-known-as-cas-and-compare-exchange">Compare-and-Swap (also known as CAS and Compare-Exchange)</a></h1>
<p>Definition: swap a value with a new value <em>only if the the current value is what
we think it is.</em></p>
<p>This reason this is important is that loosely, we can say the state during the
swap is the same as the state we observed when preparing for.</p>
<p>Here's a code example:</p>
<pre><code>LOAD A
CAS old = A, new = 2 * A // Only swap in the double if the number hasn't changed
</code></pre>
<p>A more realistic with a linked list would be this;</p>
<pre><code>LOAD LIST_NODE

CAS old = LIST_NODE.pointer, new = new_pointer
</code></pre>
<p>In this case, we switch where the <code>LIST_NODE</code> points to only if it is pointing
to where it was when we first loaded it.</p>
<p>Here's what we did:</p>
<ol>
<li>We loaded the node</li>
<li>We read the pointer</li>
<li>We CAS'd a new pointer</li>
</ol>
<p>At this point, there are two things that can happen:</p>
<ol>
<li>The pointer changed, and the CAS fails. This means that someone else changed
the pointer first, and it's good that the CAS failed, because it's possible
the the change that succeeded invalidates the change we are trying to make.</li>
<li>The pointer is the same, and CAS succeeds. Because the pointer was the same,
our assumptions about the state of the vector held, and our change was valid.</li>
</ol>
<p>At first, this might seem contrived and confusing (as it did to me). I would
focus on this intuition: <em>if CAS succeeds, loosely, we can say the state during
the swap was the same as the state we observed when preparing for the swap.</em> Our
assumptions were consistent throughout the whole process.</p>
<p>The <code>compare_exchange</code> function in the Rust Standard Library returns a
<code>Result&lt;T, T&gt;</code>, where T is the type being exchanged. The contents of the result
is the value that the variable actually was. If <code>compare_exchange</code> fails, it
returns <code>Err(actual_value)</code>, on success, it returns <code>Ok(expected_value)</code> (if it
succeeded, that means <code>actual_value == expected_value</code>).</p>
<p><strong>Note</strong>: for the rest of the book, I'm going to refer to <code>compare-and-swap</code> as
<code>compare_exchange</code>, as that is what the Rust Standard Library uses. I used
<code>compare-and-swap</code> on this page because it's very explicit about what the
operation does.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction-to-the-paper"><a class="header" href="#introduction-to-the-paper">Introduction to the Paper</a></h1>
<p>Firstly, a link to the paper can be found <a href="https://www.stroustrup.com/lock-free-vector.pdf">here</a>.</p>
<p>In their 2006 paper, Dechev et al. describe an implementation for a vector than can safely be shared across threads.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="structure-of-the-vector"><a class="header" href="#structure-of-the-vector">Structure of the vector</a></h1>
<p>This is where we begin actually working on the vector.</p>
<p>When thinking about the structure of the vector, I find it helpful to think
about it in two parts: memory and synchronization. By memory I mean allocation
of space and by synchronization I mean synchronizing reads/writes. Let's start
with memory.</p>
<h2 id="memory"><a class="header" href="#memory">Memory</a></h2>
<p>The vector is, broadly, a two-level array.</p>
<pre><code>+---+---+---+---+---+
| 1 | 2 | 3 | 4 | 5 | Top-level
+---+---+---+---+---+
  |   |   |   |   |
  v   v   v   v   v
+---+---+---+---+---+
| 1 | 2 | 3 | 4 | 5 | Lower-level, notice: these arrays are represented vertically
+---+---+---+---+---+
    | 2 | 3 | 4 | 5 |
    +---+---+---+---+
        | 3 | 4 | 5 |
        +---+---+---+
            | 4 | 5 |
            +---+---+
                | 5 |
                +---+

</code></pre>
<p>The vector stores a pointer to a first array (the top-level one). The elements
in this array are also pointers, to more arrays (the lower-level ones). This is
why this organization is called a two-level array.</p>
<p>The reason for this is resizing. Suppose we have 4 elements of capacity, and
they are all filled. We need to allocation more memory. We allocate more memory
using something like <code>malloc()</code>, and the allocator returns a pointer to the new
allocation.</p>
<p>For a normal vector, we would simply copy our vector's elements over to the new
allocation. We can't do this for a lockless vector though because copying isn't
atomic, and we can't lock down the vector, copy, and unlock. Therefore, we need
a different system.</p>
<blockquote>
<p><strong>A little tangent on allocations</strong>: When allocating memory for a normal
vector, we generally make larger and larger allocations. For example, the
first allocation could be 4 elements, the next 8, then 16, 32 . . . This
reduces the total number of allocations we need to perform, which is good for
performance. We're going to use this same idea for the vector.</p>
</blockquote>
<p>Returning to the idea of a two-level array, the first level is going to hold
pointers to blocks of memory we can call <em>buckets</em>. The amount of memory a
bucket holds is related to it's index. The first bucket will hold some constant
(which we'll call <code>FIRST_BUCKET_SIZE</code>) times 2 to the power of its index
elements. Here are some sample calculations for the first few buckets to show
the principle, using <code>FIRST_BUCKET_SIZE=8</code>:</p>
<pre><code class="language-python"># Bucket 1
CAP = FIRST_BUCKET_SIZE * 2 ^ INDEX
    = 8 * 2 ^ 0
    = 8

# Bucket 2
CAP = FIRST_BUCKET_SIZE * 2 ^ INDEX
    = 8 * 2 ^ 1
    = 16
</code></pre>
<p>The next part of the vector's structure is the synchronization aspect, but I'm
going to fully explain the algorithm first to give some context.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="synchronization"><a class="header" href="#synchronization">Synchronization</a></h1>
<p>Synchronization, that is, coordinating concurrent operations on the vector, is
achieved through two little data structures: the <code>Descriptor</code> and the
<code>WriteDescriptor</code>. As you might expect, the <code>Descriptor</code> describes the vector
and the <code>WriteDescriptor</code> describes a write operation.</p>
<h3 id="the-descriptor"><a class="header" href="#the-descriptor">The Descriptor</a></h3>
<p>The descriptor holds two values: a pointer to a <code>WriteDescriptor</code>, and a value
indicating the size/length of the vector.</p>
<h3 id="the-writedescriptor"><a class="header" href="#the-writedescriptor">The WriteDescriptor</a></h3>
<p>The WriteDescriptor holds three values: the location of the write (a
pointer-like object), an old value, and a new value. You might be wondering why
a <code>WriteDescriptor</code> holds an old value. The answer: <code>compare_exchange</code>.</p>
<p>Now that we've seen the <code>Descriptor</code> and <code>WriteDescriptor</code>, here's a quick
summary of the vector structure:</p>
<!-- prettier-ignore-start -->
<pre><code class="language-yaml"># Data Organization
Vector: 
    [Pointer -&gt; Memory],
    Pointer -&gt; Descriptor

Descriptor: 
    Pointer -&gt; Possible WriteDescriptor, 
    Size

WriteDescriptor: 
    Pointer -&gt; Element location, 
    New Element, 
    Old Element
</code></pre>
<!-- prettier-ignore-end -->
<h2 id="how-does-this-actually-help-with-synchronization"><a class="header" href="#how-does-this-actually-help-with-synchronization">How does this actually help with synchronization?</a></h2>
<blockquote>
<p>The major challenges of providing lock-free vector implementation stem from
the fact that key operations need to atomically modify two or more
non-colocated words (Dechev et. al., 2006)</p>
</blockquote>
<p>This translates to, &quot;We need to change two things (without locking the vector)
down to ensure the vector is in the right state.&quot; For a <code>push</code> operation, say,
we would need to change the <em>length</em> of the vector and write the new <em>data</em>. We
also might need to allocate more memory, which just means more changes we need
to synchronize.</p>
<p>The descriptor system gets around this by saying, &quot;If you want to change the
descriptor, you need to complete a pending write.&quot; Why does this ensure the
correct semantics? Consider this example from the paper:</p>
<blockquote>
<p>The semantics of the <code>pop_back</code> and <code>push_back</code> operations are guaranteed by
the <code>Descriptor</code> object. Consider the case when a <code>pop_back</code> is interrupted by
any matching number of <code>push_back</code> and <code>pop_back</code> operations. In a naive
implementation, the size of the vector would appear unchanged when the
original <code>pop_back</code> resumes and the operation could produce an erroneous
result.</p>
</blockquote>
<p>Under the &quot;naive implementation&quot;, in this scenario, the vector might look like
<code>[1, 2, 3]</code>. Someone calls <code>pop</code>, and the vector should return <code>3</code>. However, the
thread gets <em>preempted</em> (the OS says another thread can run, and the current
thread is paused), and the running thread executes a bunch of <code>pop</code>s and
<code>push</code>es. The vector is now <code>[4, 5, 6]</code>. When the original pop finally runs, it
returns <code>6</code>.</p>
<p>Let's consider when the first <code>push</code> happens after the original <code>pop</code> under the
correct implementation. When the <code>push</code> happens, it swaps in a new <code>Descriptor</code>,
which says that the size is now one bigger and points to a new <code>WriteDescriptor</code>
representing a <code>push</code> operation. Because it swapped in a <code>Descriptor</code>, it has to
complete the operation specified in the current <code>WriteDescriptor</code>, and the
original pop returns <code>3</code>, as it should.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-algorithm"><a class="header" href="#the-algorithm">The Algorithm</a></h1>
<p>As I‚Äôve said before, I think of the vector as two connected systems: memory and
synchronization. By ‚ÄúThe Algorithm‚Äù, I mean the synchronization aspect. To
recap, synchronization is controlled by two little data structures, the
<code>Descriptor</code> and the <code>WriteDescriptor</code>. These data structures describe the
vector itself and a write operation, respectively.</p>
<p>I think the best way to explain the algorithm is to dive right in.</p>
<p>First, I want to explain a little routine called <code>complete_write</code>. This function
is true to its name and <em>completes</em> a <em>write</em>.</p>
<blockquote>
<p>Write means &quot;write operation&quot; in this context, a push or pop. In my
experience, &quot;write&quot; has been a more colloquial term used in CS for whenever we
make a modification to something. Really anything can technically be a
&quot;write&quot;, but I would say things that are more final are &quot;writes&quot;. For example,
incrementing a loop variable is pretty insignificant in the grand scheme of
things, so it's not really a &quot;write&quot;, but increasing the size of the vector is
an important &quot;write&quot;. This usage might also be particular to concurrent
programming, where balancing reads/writes is an important consideration for
designing a data structure. Most data structures are designed for infrequent
writes and frequent reads. Modifications to databases (which are <strong>heavily</strong>
concurrent) can also be called writes. tl;dr a &quot;write&quot; in this case means the
details describing a particular instance of &quot;writing&quot;</p>
</blockquote>
<p><code>complete_write</code> takes two arguments, a <code>WriteDescriptor</code>, and the vector
itself. <code>complete_write</code> applies the write operation described in the
<code>WriteDescriptor</code> on the vector. Recall that a <code>WriteDescriptor</code> contains three
things: a reference/pointer to the location where the write will take place, a
new value to write, and an old value that we load in from the location. If your
spidey-sense is tingling, it might be because this new/old business is hinting
at a <code>compare_exchange</code> in the future. Your spidey-sense is correct.</p>
<p>First we perform a <code>compare_exchange</code> using the data in the <code>WriteDescriptor</code>,
we only swap in the new data if the data at the location of the swap matches the
old data we have. If the <code>compare_exchange</code> succeeds, this means that we swapped
in the value we want to write. If it fails, it means someone else beat us to it,
and performed the write. Remember, many threads can access the vectors
<code>Descriptor</code> and <code>WriteDescriptor</code> at once, so many threads will be trying to
complete the same write. Only one of them can succeed. It's a fight to the
death! Arrghhh!!!</p>
<p>I'm kidding. After performing the <code>compare_exchange</code>, successful for not, we
modify the vector to indicate that there is no pending write operation. If all
threads do this, at least once will succeed, and all will indicate that there is
no pending write operations. Though some of the threads may be sad because their
<code>compare_exchange</code> failed, the vector is happy because it's in a consistent and
correct state.</p>
<p>And that's all there is to <code>complete_write</code>. Now that we know writes are
actually performed, let‚Äôs get into how a <code>push</code> operation works. Here are the
steps:</p>
<ol>
<li>
<p>Load in the current descriptor.</p>
</li>
<li>
<p>If the descriptor contains a write operation, complete it . This is important
because it ensures that before any new write operation happens, the previous
one is completed. We cannot do anything before completing the previous write
operation, so all operations <em>will</em> eventually get executed.</p>
</li>
<li>
<p>Calculate which bucket our new element will go into.</p>
</li>
<li>
<p>If that bucket has not been allocated memory yet, do so.</p>
</li>
<li>
<p>Make a new <code>WriteDescriptor</code>. The <code>new</code> value in the <code>WriteDescriptor</code> will
be the data passed into the <code>push</code> function.</p>
</li>
<li>
<p>Make a new <code>Descriptor</code> which contains the following data: the size held in
the current <code>Descriptor</code> + 1, and the new <code>WriteDescriptor</code>.</p>
</li>
<li>
<p>Now, here comes the part that makes this a <code>compare-and-swap</code> or
<code>compare_exchange</code> algorithm. We then <code>compare_exchange</code> the new <code>Descriptor</code>
we made with the old one. If the <code>Descriptor</code> held in the vector didn't
change, our new <code>Descriptor</code> will replace it. If it did change, we will fail
to swap in our new <code>Descriptor</code>, and we go back to Step 1.</p>
<blockquote>
<p>Note: I think it's important to consider why this routine (particularly
step 6) ensures correctness. If the <code>compare_exchange</code> succeeds, this
means that the vector did not change in the time it took us to prepare a
new <code>Descriptor</code>. Why is this important? It means our assumptions about
the vector's state <strong>did not change</strong>. In our new <code>Descriptor</code>, we used
the size from the <code>Descriptor</code> we loaded in, and incremented that by one.
So, if the size we loaded in was <code>4</code>, our new <code>Descriptor</code> would say the
size of the vector is <code>5</code>. Now, imagine that we could just swap in our
fresh <code>Descriptor</code> without comparing it with the current one. If someone
else was also trying to <code>push</code>, their <code>Descriptor</code> might get swapped in
before ours. It would say the size of the vector is <code>5</code>, because it made
the same assumptions we did. Then we swap in our <code>Descriptor</code>, our
<code>Descriptor</code> would maintain that the size of the vector is <code>5</code>, even
though it should be <code>6</code> because there were two <code>push</code> operations.
Furthermore, we would overwrite the element that was <code>push</code>ed on by the
first call to push, because both our <code>WriteDescriptor</code>s would be
referencing the same location in memory. This is terrible!
<code>compare_exchange</code> is our friend.</p>
</blockquote>
</li>
<li>
<p>Now that we have swapped in our <code>Descriptor</code>, we execute the
<code>WriteDescriptor</code> we made using <code>complete_write</code>, finalizing the changes we
want to make to the vector.</p>
</li>
</ol>
<p>And that's a <code>push</code>!</p>
<p><code>Pop</code> pretty much works the same except for some small variations, so we'll get
into that when we implement <code>push</code>/<code>pop</code>. However, the way we make sure changes
are valid using <code>compare_exchange</code> is identical for both operations.</p>
<p>I think it's finally time to start looking at some code. When I was writing
code, it felt very different from reasoning about the theory. I really felt like
I had to consider every line I wrote and decision I made. I'll now walk you
through what I came up with.</p>
<blockquote>
<p>Note: we're going to first write a version of the vector that doesn't reclaim
memory; it <em>leaks</em>.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="starting-code"><a class="header" href="#starting-code">Starting Code</a></h1>
<h2 id="pseudocode"><a class="header" href="#pseudocode">Pseudocode</a></h2>
<p>This &quot;pythonesque&quot; pseudocode with some pointer operations thrown in shows the
general API and implementation details of the vector. The pseudocode is a
conversion of the paper's pseudocode into a more (in my opinion) understandable
form. It completely ignores memory reclamation.</p>
<p>You don't need to read this entire thing, it's just here as a reference.</p>
<pre><code class="language-python"># Calculate the index of the correct bucket
# Return a pointer
def at(vector, i):
    pos = i + FIRST_BUCKET_SIZE
    hibit = highest_bit(pos)
    index = pos ^ 2 ** hibit
    return &amp;vector.memory[hibit - highest_bit(FIRST_BUCKET_SIZE)][index]
</code></pre>
<pre><code class="language-python"># Perform an atomic load at the correct index
def read(vector, i):
    return *at(vector, i).load(Ordering)
</code></pre>
<pre><code class="language-python"># Perform an atomic store at the correct index
def write(vector, i, elem):
    return *at(vector, i).store(elem, Ordering)
</code></pre>
<pre><code class="language-python"># Calculate the number of allocations needed
# Then perform each allocation
def reserve(vector, size):
    i = highest_bit(vector.descriptor.size + FIRST_BUCKET_SIZE - 1)
        - highest_bit(FIRST_BUCKET_SIZE)
    if i &lt; 0 {
        i = 0
    }
    while i &lt; highest_bit(size + FIRST_BUCKET_SIZE - 1)
        - highest_bit(FIRST_BUCKET_SIZE):
        i += 1
        allocate_bucket(vector, i)
</code></pre>
<pre><code class="language-python"># Calculate the amount of memory needed
# Allocate that much memory
# Try to CAS it in
# If CAS fails, the bucket is already initalized, so free the memory
def allocate_bucket(vector, bucket):
    bucket_size = FIRST_BUCKET_SIZE * (2 ** bucket)
    mem = allocate(bucket_size)
    if not CAS(&amp;vector.memory[bucket], nullptr, mem):
        free(mem)
</code></pre>
<pre><code class="language-python"># Get the size of the current descriptor
# If there is a pending write operation, subtract one from the size
def size(vector):
    size = vector.descriptor.size
    if descriptor.writeop.pending:
        size -= 1
    return size
</code></pre>
<pre><code class="language-python"># Get the current descriptor
# Complete a pending write operation
# Allocate memory if needed
# Make a new WriteDescriptor
# Try to CAS it in
# If CAS failed go back to first step
# Complete a pending write operation
def push(vector, elem):
    while True:
        current_desc = vector.descriptor
        complete_write(vector, current_desc.pending)
        bucket = highest_bit(current_desc.size + FIRST_BUCKET_SIZE)
            - highest_bit(FIRST_BUCKET_SIZE)
        if vector.memory[bucket] == nullptr:
            allocate_bucket(vector, bucket)
        writeop = WriteDescriptor(
            *at(vector, current_desc.size),
            elem,
            current_desc.size
        )
        next_desc = Descriptor(1 + current_desc.size, writeop)
        if CAS(&amp;vector.descriptor, current_desc, next_desc):
            break
    complete_write(vector, next_desc.pending)
</code></pre>
<pre><code class="language-python"># Get the current descriptor
# Complete a pending write operation
# Read the last element of the vector
# Make a new WriteDescriptor
# Try to CAS it in
# If CAS failed go back to first step
# Return the last element
def pop(vector):
    while True:
        current_desc = vector.descriptor
        complete_write(vector, current_desc.pending)
        elem = *at(current_desc.size - 1).load(Ordering)
        next_desc = Descriptor(curr_desc.size - 1, null)
        if CAS(&amp;vector.descriptor, current_desc, next_desc):
            break
    return elem
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-allocation"><a class="header" href="#memory-allocation">Memory Allocation</a></h1>
<p>The first thing I did when writing the code out was think about the pieces that
would make up the vector. In Rust, an extremely common building block for any
type is the <code>struct</code>. A <code>struct</code> just sticks its members' data next to each
other in memory. Here is the vector itself, as a struct:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SecVec&lt;'a, T: Sized + Copy&gt; {
    buffers: CachePadded&lt;Box&lt;[AtomicPtr&lt;AtomicU64&gt;; 60]&gt;&gt;,
    descriptor: CachePadded&lt;AtomicPtr&lt;Descriptor&lt;'a, T&gt;&gt;&gt;,
    _boo: PhantomData&lt;T&gt;, // Data is stored as transmuted T's
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="boo-"><a class="header" href="#boo-">Boo! üëª</a></h2>
<p>I bet the PhantomData scared you. We have a generic parameter <code>T</code>, but we have no
<code>struct</code> members of <code>SecVec</code> or either of the descriptors that actually contains
a T (because we transmute T into <code>u64</code>s). Therefore, to let the compiler know we
really are carrying T's, we add a little ghost that tells it, &quot;Here is a Phantom
T that we're carrying.&quot;</p>
<h2 id="cache"><a class="header" href="#cache">Cache</a></h2>
<p>There is a lot to unpack here. Firstly, <code>CachePadded</code> is a <code>struct</code> provided by
the crate <code>crossbeam_utils</code> crate.</p>
<blockquote>
<p>A note on cache: you may have heard of CPU cache, a small buffer of memory
stored on the CPU to allow for fast access. The <code>cache</code> in <code>CachePadded</code>
actually refers to a buffer between main RAM and the CPU's. It's just a larger
and slower cache compared to a CPU cache. The cache is split into contiguous
blocks of memory called <em>cache lines</em>. This is the most granular level at
which cache coherency is maintained. When multiple threads both have a value
in the same cache line, one thread modifying its value marks the cache line as
&quot;dirty&quot;. Even though the other thread's value hasn't been changed, the cache
coherency protocol might cause the thread to reload the entire line when it
uses the value, incurring some overhead. This can cause severe performance
degradation. Cache is a really important consideration when data structures.
It's why linked lists are algorithmically fine but terribly slow in practice.
As the saying goes, cache is king.</p>
</blockquote>
<p>The <code>CachePadded</code> <code>struct</code> aligns its contents to the beginning of the cache
line to prevent false sharing. If all <code>CachePadded</code> objects are at the beginning
of a cache line (assuming they do not cross a cache line), there can't be false
sharing between them. Preventing false sharing can lead to a huge speedup, but
it also does increase the size of the type. If you're wondering how
<code>CachePadded</code> is implemented, check out
<a href="https://doc.rust-lang.org/nomicon/other-reprs.html"><code>#[repr(align(n))]</code></a> in the
Nomicon.</p>
<p>Here's how I picture cache padding:</p>
<pre><code>|-----Cache line-----|-----Cache Line-----|
v                    v                    v
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
|69|xx|xx|xx|xx|xx|xx|42|xx|xx|xx|xx|xx|xx|
+--+--+--+--+--+--+--+--+--+--+--+--+--+--+
^                    ^
 \                    \
  \                    \
   \                    \
    Different cache lines
</code></pre>
<h2 id="two-level-array"><a class="header" href="#two-level-array">Two-level array</a></h2>
<p>The first member of <code>SecVec&lt;T&gt;</code> is a cache-padded array of 60 pointers allocated
on the heap (notice the <code>Box</code>). These pointers will each point into another
array. The pointers start off as null pointers (<code>0x0</code>), and will get swapped out
for valid pointers once they need to point to an actual array.</p>
<p>The <code>AtomicPtr</code>s point to <code>AtomicU64</code>s because each element is going to get
transmuted into a <code>u64</code> so that we can atomically perform writes on the vector.
When returning an element, we'll transmute it back into a T.</p>
<h2 id="descriptors-galore"><a class="header" href="#descriptors-galore">Descriptors galore</a></h2>
<p>The second member of <code>SecVec&lt;T&gt;</code> is a cache-padded <code>AtomicPtr</code> to a
<code>Descriptor</code>. As you've probably noticed, there are a bunch of <code>AtomicPtr</code>s
here. That's because we can modify the pointer atomically, specify which
<code>Ordering</code> to use, and <code>compare_exchange</code> the pointer. A common way of writing
data in concurrent programming is to change a pointer instead of actually
modifying a buffer. Since a buffer can't necessarily be modified atomically or
without locking, what we can do is prepare a buffer and then change a pointer so
that it points to our new buffer. All new readers will see the new data when
they dereference the pointer.</p>
<p>What do we do with the old pointer you might ask? Worry not, we will get into
that üòÖ</p>
<h3 id="the-descriptor-and-writedescriptor"><a class="header" href="#the-descriptor-and-writedescriptor">The Descriptor and WriteDescriptor</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct Descriptor&lt;'a, T: Sized&gt; {
    pending: AtomicPtr&lt;Option&lt;WriteDescriptor&lt;'a, T&gt;&gt;&gt;,
    size: usize,
}

pub struct WriteDescriptor&lt;'a, T: Sized&gt; {
    new: u64,
    old: u64,
    location: &amp;'a AtomicU64,
    _marker: PhantomData&lt;T&gt;, // New and old are transmuted T's
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="the-trait-bounds"><a class="header" href="#the-trait-bounds">The trait bounds</a></h2>
<p>Notice how T is <code>Sized</code>, this means that its size is always known at
compile-time. We need to ensure this because our values need to be transmutable.
Part of the safety contract of <code>transmute_copy</code> is making sure our types are of
compatible sizes.</p>
<p>The <code>Copy</code> bound is necessary because the data in the vector is copied in and
out of the buffers, with <code>transmute_copy</code>.</p>
<p>OK, enough talk about <code>struct</code>s, let's get to the first function: <code>get()</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="get"><a class="header" href="#get"><code>get()</code></a></h1>
<p>The first, and simplest function to write is <code>vector.get(i)</code>, which returns a
pointer to the element at index <em>i</em>.</p>
<p>This is the code I wrote:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Return a *const T to the index specified
///
/// # Safety
/// The index this is called on **must** be a valid index, meaning:
/// there must already be a bucket allocated which would hold that index
/// **and** the index must already have been initialized with push/set
unsafe fn get(&amp;self, i: usize) -&gt; *const AtomicU64 {
    // Check for overflow
    let pos = i
        .checked_add(FIRST_BUCKET_SIZE)
        .expect(&quot;index too large, integer overflow&quot;);

    let hibit = highest_bit(pos);

    let offset = pos ^ (1 &lt;&lt; hibit);

    // Select the correct buffer to index into
    // # Safety
    // Since hibit = highest_bit(pos), and pos &gt;= FIRST_BUCKET_SIZE
    // The subtraction hibit - highest_bit(FIRST_BUCKET_SIZE) cannot underflow
    let buffer = &amp;self.buffers[(hibit - highest_bit(FIRST_BUCKET_SIZE)) as usize];

    // Check that the offset doesn't exceed isize::MAX
    assert!(
        offset
            .checked_mul(mem::size_of::&lt;T&gt;())
            .map(|val| val &lt; isize::MAX as usize)
            .is_some(),
        &quot;pointer offset exceed isize::MAX bytes&quot;
    );

    // Offset the pointer to return a pointer to the correct element
    unsafe {
        // # Safety
        // We know that we can offset the pointer because we will have allocated a
        // bucket to store the value. Since we only call values that are
        // `self.descriptor.size` or smaller, we know the offset will not go out of
        // bounds because of the assert.
        buffer.load(Ordering::Acquire).add(offset)
    }
}
<span class="boring">}
</span></code></pre></pre>
<h2 id="a-few-points-to-note"><a class="header" href="#a-few-points-to-note">A few points to note</a></h2>
<p>Noticed how I've marked the function as <code>unsafe</code>. This is because there is a
safety contract the compiler can't enforce: the index must be valid. This is
automatically guaranteed through the usage of the function in the algorithm, but
I marked it <code>unsafe</code> just to be explicit.</p>
<p>The function is pretty straightforward: we calculate which buffer the item is
in, load the pointer to the start of the buffer, and offset it to the correct
element. There are two things I want to point out. First, notice all the checks
we make to avoid overflow. Secondly, notice the use of <code>Ordering::Acquire</code> for
loading in the pointer to the buffer. Since we always store the pointer with
<code>Ordering::Release</code>, we are guaranteed to get the most recent pointer, because
an <code>Acquire</code> load cannot get ordered before a <code>Release</code> store! I find it very
satisfying how <code>Acquire</code> and <code>Release</code> work together. It's like two puzzle
pieces fitting nicely into each other</p>
<h2 id="what-are-all-these-bitwise-operations"><a class="header" href="#what-are-all-these-bitwise-operations">What are all these bitwise operations?</a></h2>
<p>TODO</p>
<p>Next up is the <code>allocate_bucket</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="allocate_bucket"><a class="header" href="#allocate_bucket"><code>allocate_bucket()</code></a></h1>
<p>Remember that whole &quot;two-level array&quot; thingy? This is where it starts coming
into play. <code>allocate_bucket</code> does just what is sounds like: allocating a bucket.
Recall that a bucket is one of the arrays in the <em>second</em> level of the two level
array.</p>
<pre><code>+---+---+---+---+---+
| 1 | 2 | 3 | 4 | 5 |
+---+---+---+---+---+
  |   |   |   |   |
  v   v   v   v   v
+---+---+---+---+---+
| 1 | 2 | 3 | 4 | 5 |
+---+---+---+---+---+
    | 2 | 3 | 4 | 5 |
    +---+---+---+---+
        | 3 | 4 | 5 |
        +---+---+---+
          ^ | 4 | 5 |
          | +---+---+
          |     | 5 |
          |     +---+
          |
        we're allocating one of these little guys
</code></pre>
<p>There are two parts to <code>allocate_bucket</code>: allocating the memory and setting the
pointer. We start off by tapping into the <code>alloc</code> crate's API. First, we create
a <code>Layout</code>, which describes the allocation we want. The
<code>Layout::array::&lt;Atomic64&gt;()</code> function indicates that we want an array of
<code>AtomicU64</code> <code>size</code> long. If creating the layout fails (due to overflow), we call
<code>capacity_overflow</code>, which just panics.</p>
<blockquote>
<p>You might ask, why not just directly call <code>panic!</code>? Apparently, it reduces the
generated code size if we just have panic in one function, which we then call
from multiple places. I found this trick in the source code for
<a href="https://github.com/rust-lang/rust/blob/master/library/alloc/src/raw_vec.rs#L512-L518"><code>std::vec::Vec</code></a>.
You can learn a lot from reading the Standard Library code.</p>
</blockquote>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>const FIRST_BUCKET_SIZE: usize = 8;

fn allocate_bucket(&amp;self, bucket: usize) {
    // The shift-left is equivalent to raising 2 to the power of bucket
    let size = FIRST_BUCKET_SIZE * (1 &lt;&lt; bucket);
    let layout = match Layout::array::&lt;AtomicU64&gt;(size) {
        Ok(layout) =&gt; layout,
        Err(_) =&gt; capacity_overflow(),
    };

<span class="boring">}
</span></code></pre></pre>
<p>The next thing we do is just another check. The Standard Library does both and I
trust their
<a href="https://github.com/rust-lang/rust/blob/master/library/alloc/src/raw_vec.rs#L176-L183">strategy</a>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Make sure allocation is ok
match alloc_guard(layout.size()) {
    Ok(_) =&gt; {}
    Err(_) =&gt; capacity_overflow(),
}

<span class="boring">}
</span></code></pre></pre>
<blockquote>
<p><a href="https://github.com/rust-lang/miri"><code>Miri</code></a> is about to make its debut! <code>Miri</code>
is a tool that runs your code in a special environment and detects undefined
behavior (or UB, as the cool kids call it).</p>
</blockquote>
<p>Now that our layout is all good, we can perform the actual allocation. We
instantiate the <code>Global</code> <code>struct</code>, which is the allocator we're using. The
allocator returns a pointer to our new allocation once it's finished allocating.
Why are we using <code>allocated_zeroed</code> you might ask? Why not just allocate
normally? The answer: <em>It's Utmost Holiness:</em> <code>Miri</code>. In all seriousness though,
<code>Miri</code> has been and invaluable tool in catching memory and concurrency bugs.
When we just allocate normally, <code>Miri</code> throws and error when we start actually
using the memory later on, saying that
<code>intrinsics::atomic_cxchg_acqrel_failrelaxed(dst, old, new)</code> requires
initialized data. Thus, we just zero the memory for now. Later, it might be
worth it to do some <code>MaybeUninit</code> magic, but honestly, I don't know if there'll
be much, if any, performance gains.</p>
<p>Once again, we have more checks, and we'll just <code>panic!</code> if the allocation
fails. <code>handle_alloc_error</code> is from the <code>alloc</code> crate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let allocator = Global;

let allocation = allocator.allocate_zeroed(layout);
let ptr = match allocation {
    Ok(ptr) =&gt; ptr.as_ptr() as *mut AtomicU64,
    Err(_) =&gt; handle_alloc_error(layout),
};

<span class="boring">}
</span></code></pre></pre>
<p>The final part is to swap in the pointer into our array of buffers (the first
level of the two-level array). We use the <code>compare_exchange</code> function, with a
null pointer as the expected value, and our new pointer from the allocation. If
<code>compare_exchange</code> fails, that means the pointer is no longer null, and someone
else <code>compare_exchanged</code>ed in a pointer. Therefore, the bucket is already
allocated. In this case, we deallocate the freshly allocated memory. Notice how
we assess the result of <code>compare_exchange</code> with <code>Result::is_err()</code>; we don't
care about the value <code>compare_exchange</code> returns.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    if self.buffers[bucket] // &lt;- this is an AtomicPtr&lt;AtomicU64&gt;
        .compare_exchange(
            ptr::null_mut::&lt;AtomicU64&gt;(), // old value
            ptr, // new value
            Ordering::AcqRel, // ordering on success
            Ordering::Relaxed, // ordering on fail
        )
        .is_err()
    {
        unsafe {
            // # Safety
            // We know that the pointer returned from the allocation is NonNull so
            // we can call unwrap() on NonNull::new(). We also know that the pointer
            // is pointing to the correct memory because we just got it from the
            // allocation. We know the layout is valid, as it is the same layout we
            // used to allocate.
            allocator.deallocate(NonNull::new(ptr as *mut u8).unwrap(), layout);
        }
    }
}

<span class="boring">}
</span></code></pre></pre>
<h2 id="success-and-fail-orderings"><a class="header" href="#success-and-fail-orderings">Success and Fail Orderings</a></h2>
<p>Like all atomic operations, <code>compare_exchange</code> uses the <code>Ordering</code>s. Most
operations take 1, but this bad boy takes two. Since <code>compare_exchange</code> reads
and writes a memory location, I'm using <code>Ordering::AcqRel</code>. Since we always use
<code>AcqRel</code> for the buckets, the load part (<code>Acquire</code>) of the <code>compare_exchange</code>
will always see the most recent value because the store part is <code>Release</code>. If we
just used <code>Acquire</code>, the store part of the <code>compare_exchange</code> would be
<code>Relaxed</code>, which doesn't guarantee that the modification to the
<code>AtomicPtr&lt;AtomicU64&gt;</code> is published to other threads by any certain point. Under
a <code>Relaxed</code> situation, another thread might load a null pointer in its
<code>compare_exchange</code>, even though our thread swapped in a pointer to memory!</p>
<p>That's the success ordering. The fail ordering is <code>Relaxed</code> because we don't
need to establish any synchronization if the operation fails. It failed; we're
not doing any stores. When I first saw this, I had the question, &quot;Why do we
provide different success and fail orderings if the <code>compare_exchange</code> doesn't
know if it will fail or not?&quot; The answer, thanks to Alice on the Rust User
Forums, is that compiler picks an ordering that will always satisfy the stronger
ordering. Thus, <code>compare_exchange(success: AcqRel, fail: Release)</code> executes as
<code>compare_exchange(success: AcqRel, fail: Acquire)</code> to ensure that the initial
load is <code>Acquire</code> for both cases.</p>
<p>There's a little more to it; if you're still curious, see this
<a href="https://users.rust-lang.org/t/what-does-the-compare-exchange-fail-ordering-mean/75791">thread</a>
on the Rust User Forums.</p>
<p>The last function in the &quot;memory&quot; section is <code>reserve()</code></p>
<hr />
<h3 id="complete-source-for-allocate_bucket"><a class="header" href="#complete-source-for-allocate_bucket">Complete source for <code>allocate_bucket()</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn allocate_bucket(&amp;self, bucket: usize) {
    // The shift-left is equivalent to raising 2 to the power of bucket
    let size = FIRST_BUCKET_SIZE * (1 &lt;&lt; bucket);
    let layout = match Layout::array::&lt;AtomicU64&gt;(size) {
        Ok(layout) =&gt; layout,
        Err(_) =&gt; capacity_overflow(),
    };

    // Make sure allocation is ok
    match alloc_guard(layout.size()) {
        Ok(_) =&gt; {}
        Err(_) =&gt; capacity_overflow(),
    }

    let allocator = Global;
    // allocate_zeroed because miri complains about accessing uninitialized memory
    // TODO: Maybe use MaybeUninit?
    let allocation = allocator.allocate_zeroed(layout);
    let ptr = match allocation {
        Ok(ptr) =&gt; ptr.as_ptr() as *mut AtomicU64,
        Err(_) =&gt; handle_alloc_error(layout),
    };

    // If the CAS fails, then the bucket has already been initalized with memory
    // and we free the memory we just allocated
    if self.buffers[bucket]
        .compare_exchange(
            ptr::null_mut::&lt;AtomicU64&gt;(),
            ptr,
            Ordering::AcqRel,
            Ordering::Relaxed,
        )
        .is_err()
    {
        unsafe {
            // # Safety
            // We know that the pointer returned from the allocation is NonNull so
            // we can call unwrap() on NonNull::new(). We also know that the pointer
            // is pointing to the correct memory because we just got it from the
            // allocation. We know the layout is valid, as it is the same layout we
            // used to allocate.
            allocator.deallocate(NonNull::new(ptr as *mut u8).unwrap(), layout);
        }
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reserve"><a class="header" href="#reserve"><code>reserve()</code></a></h1>
<p>The goal of <code>reserve(n)</code> is simple: allocate enough memory to perform n pushes
without allocating more memory.</p>
<p>This is a useful function, because, as we've seen, <code>allocate_bucket</code> requires
some heavy atomics, like <code>compare_exchange</code>. If we can do our allocations in a
calmer scenario with less contention, we'll experience some performance gains.</p>
<p>We start by calculating the number of allocations we'll need to perform to
reserve enough space. The calculation is a little funky, and there's an edge
case where it can't distinguish between 0 and sizes between 1 and
<code>FIRST_BUCKET_SIZE</code>. That's why we need to explicitly allocate the first bucket.
We'll see the implementation of <code>size()</code> later, but it does use some atomic
synchronization, so we just cache it the result so we don't have to keep
incurring atomic overhead over and over.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn reserve(&amp;self, size: usize) {
    // Cache the size to prevent another atomic op from due to calling `size()` again
    let current_size = self.size();
    if current_size == 0 {
        self.allocate_bucket(0);
    }

<span class="boring">}
</span></code></pre></pre>
<p>Now, we calculate the number of allocations we've made:</p>
<p>The highest bit function returns the highest set bit in a number. A bit is set
if it's equal to one. The highest set bit of 7 (<code>0b111</code>), for example, is 2
(0-indexed). Since the buckets are increasing by a factor of two each time, the
highest set bit of the indices in each bucket is one greater than the highest
set bit of the indices in the previous bucket. Therefore, by using the highest
bit of a number in conjunction with <code>FIRST_BUCKET_SIZE</code>, we can figure out how
many allocations are needed for a certain capacity. I know I'm waving my hands a
little; I haven't taken the time to rigorously understand the arithmetic, as
it's not that interesting to me, and in practice it works.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut num_current_allocs =
    highest_bit(current_size.saturating_add(FIRST_BUCKET_SIZE) - 1)
        .saturating_sub(highest_bit(FIRST_BUCKET_SIZE));

<span class="boring">}
</span></code></pre></pre>
<p>Then we calculate the number of allocations we need to reserve the space, and
for each allocation missing, we allocate.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Compare with the number of allocations needed for size `new`
    while num_current_allocs
        &lt; highest_bit(size.saturating_add(FIRST_BUCKET_SIZE) - 1)
            .saturating_sub(highest_bit(FIRST_BUCKET_SIZE))
    {
        num_current_allocs += 1;
        self.allocate_bucket(num_current_allocs as usize);
    }
}

<span class="boring">}
</span></code></pre></pre>
<p>And that's it for memory. We can now do every thing we need to do to access and
manage the vector's memory. Now's time for the really hard part: actually implementing
the vector's functions.</p>
<hr />
<h3 id="complete-source-for-reserve"><a class="header" href="#complete-source-for-reserve">Complete source for <code>reserve()</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn reserve(&amp;self, size: usize) {
    // Cache the size to prevent another atomic op from due to calling `size()` again
    let current_size = self.size();
    if current_size == 0 {
        self.allocate_bucket(0);
    }

    // Number of allocations needed for current size
    let mut num_current_allocs =
        highest_bit(current_size.saturating_add(FIRST_BUCKET_SIZE) - 1)
            .saturating_sub(highest_bit(FIRST_BUCKET_SIZE));

    // Compare with the number of allocations needed for size `new`
    while num_current_allocs
        &lt; highest_bit(size.saturating_add(FIRST_BUCKET_SIZE) - 1)
            .saturating_sub(highest_bit(FIRST_BUCKET_SIZE))
    {
        num_current_allocs += 1;
        self.allocate_bucket(num_current_allocs as usize);
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operations"><a class="header" href="#operations">Operations</a></h1>
<p>Now that we have a solid memory backbone, we're going to implement the public
API of the vector: <code>new</code>, <code>push</code>, <code>pop</code>, and <code>size</code>, as well as <code>complete_write</code>
and some helper functions.</p>
<p>Since it's been a little bit, here's what the vector look like in code form again:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SecVec&lt;'a, T: Sized + Copy&gt; {
    buffers: CachePadded&lt;Box&lt;[AtomicPtr&lt;AtomicU64&gt;; 60]&gt;&gt;,
    descriptor: CachePadded&lt;AtomicPtr&lt;Descriptor&lt;'a, T&gt;&gt;&gt;,
    _marker: PhantomData&lt;T&gt;, // Data is stored as transmuted T's
}

struct Descriptor&lt;'a, T: Sized&gt; {
    pending: AtomicPtr&lt;Option&lt;WriteDescriptor&lt;'a, T&gt;&gt;&gt;,
    size: usize,
}

struct WriteDescriptor&lt;'a, T: Sized&gt; {
    new: u64,
    old: u64,
    location: &amp;'a AtomicU64,
    _marker: PhantomData&lt;T&gt;, // New and old are transmuted T's
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="new"><a class="header" href="#new"><code>new()</code></a></h1>
<p>We've got to have some way of making a vector (or at least for an outside user
to make one).</p>
<p>What are the ingredients we need to make the vector? Buffers, <code>Descriptor</code>, and
<code>WriteDescriptor</code>. The <code>WriteDescriptor</code> is going to be <code>None</code>, as we don't have
any pending writes yet.</p>
<p>Here's the code:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// We actually do want this to be copied
#[allow(clippy::declare_interior_mutable_const)]
const ATOMIC_NULLPTR: AtomicPtr&lt;AtomicU64&gt;
    = AtomicPtr::new(ptr::null_mut::&lt;AtomicU64&gt;());

pub fn new() -&gt; Self {
    // Make an array of 60 AtomicPtr&lt;Atomicu64&gt; set to the null pointer
    let buffers = Box::new([ATOMIC_NULLPTR; 60]);

    // Make a new WriteDescriptor
    let pending = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();

    // Make a new descriptor
    let descriptor = Descriptor::&lt;T&gt;::new_as_ptr(pending, 0, 0);

    // Return self
    Self {
        descriptor: CachePadded::new(AtomicPtr::new(descriptor)),
        buffers: CachePadded::new(buffers),
        _boo: PhantomData,
    }
}

<span class="boring">}
</span></code></pre></pre>
<p>Firstly, we declare this constant, <code>ATOMIC_NULLPTR</code>. This is just an <code>AtomicPtr</code>
containging a null pointer. The reason the <code>const</code> declaration is necessary is
that when we make an array of something <code>[SOMETHING; 60]</code>, that <code>SOMETHING</code>
needs to be <code>Copy</code> or evaluatable at compile time. Since <code>AtomicPtr&lt;AtomicU64&gt;</code>
is not <code>Copy</code>, we resort to creating <code>ATOMIC_NULLPTR</code> at compile time. Once we
have our array of null pointers, we put it on the heap to reduce the size of the
vector. If we were carrying it all directly, the vector would be over 480 bytes
large! With a <code>Box</code>, we only store 8 bytes for the first level in our two-level
array.</p>
<p>Then, we make a <code>WriteDescriptor</code> using <code>new_none_as_ptr()</code>, which returns an
<code>Option&lt;WriteDescriptor&lt;T&gt;&gt;</code>. We pass this into the constructor (<code>new_as_ptr</code>)
for <code>Descriptor&lt;T&gt;</code>, and then assemble the <code>Descriptor</code> and the <code>Box</code>ed array
together to make the vector.</p>
<p>The constructors for the descriptor types end in <code>as_ptr</code> because they actually
return a raw pointer pointing to a heap allocation containing the value. We
achieve this by making a <code>Box</code> and then extracting the inner raw pointer.</p>
<pre><code>let b = Box::(5);
let b_ptr = Box::into_raw(b); &lt;- That's a raw pointer to heap memory!
</code></pre>
<h2 id="my-first-ub-mistake"><a class="header" href="#my-first-ub-mistake">My first UB mistake</a></h2>
<p>I introduced the heap and the stack earlier in the keywords section, but I
didn't explain why the distinction is important.</p>
<p>When a function is called, a <em>stack frame</em> is pushed onto the stack. This stack
frame contains all the function's local variables. When the function returns,
the stack frame is popped off the stack, and all local variables are destroyed.
This would also invalidate all references to local variables.</p>
<p>The heap is different. You allocate on the heap, and you deallocate on the heap.
Nothing happens automatically. This is the legendary <code>malloc/free</code> combo from
<code>C</code>.</p>
<p>Understanding the distinction between the stack and the heap is important
because we are using raw pointers, which don't have the guarantees of
references.</p>
<p>Here is my first mistake, summarized a little:</p>
<pre><pre class="playground"><code class="language-rust">use core::sync::atomic::{Ordering, AtomicPtr};

fn main() {
    let ptr = new_descriptor();
    let d = unsafe { &amp;*ptr.load(Ordering::Acquire) };
}

fn new_descriptor() -&gt; AtomicPtr&lt;Descriptor&gt; {
    let d = Descriptor { size: 0, write: None };
    AtomicPtr::new(&amp;d as *const _ as *mut _)
}

struct Descriptor {
    size: usize,
    write: Option&lt;bool&gt;
}

</code></pre></pre>
<pre><code>$ cargo miri run
</code></pre>
<pre><code>error: Undefined Behavior: pointer to alloc1184 was dereferenced after this allocation got freed
  --&gt; src\main.rs:46:22
   |
46 |     let d = unsafe { &amp;*ptr.load(Ordering::Acquire) };
   |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ pointer to alloc1184 was dereferenced after this allocation got freed
   |
   = help: this indicates a bug in the program: it performed an invalid operation, and caused Undefined Behavior
</code></pre>
<p><code>Miri</code> says
<code>pointer to alloc1184 was dereferenced after this allocation got freed</code>.
Translation: <code>use-after-free</code>; classic UB.</p>
<p>So why is the <code>Descriptor</code>'s allocation being freed? Because it's <strong>allocated on
the stack</strong>. When <code>new_descriptor</code> returns, the local variable <code>d: Descriptor</code>
get's destroyed, and the pointer we made from the reference is invalidated.
Thus, the <code>use-after-free</code> when we deference a freed allocation.</p>
<p>This is the danger of using raw pointers. With references, Rust will keep the
value alive until there are no references to it and it's safe to drop at the end
of a scope.</p>
<p>This example also highlights why only dereferencing a raw pointer is <code>unsafe</code>.
It's perfectly safe to make one, but we have no guarantees about what it's
pointing to, and that's why the dereference is <code>unsafe</code>.</p>
<p>Thank you <code>Miri</code>!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="complete_write"><a class="header" href="#complete_write"><code>complete_write()</code></a></h1>
<p>I think <code>complete_write</code> is the first function I wrote for the vector's core
operations. We execute the <code>WriteDescriptor</code> passed in and set the one stored in
the vector to <code>None</code></p>
<p>Here's what the function signature looks like:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn complete_write(&amp;self, pending: &amp;Option&lt;WriteDescriptor&lt;T&gt;&gt;) {

<span class="boring">}
</span></code></pre></pre>
<p>The first thing we do is execute the <code>WriteDescriptor</code>, if there is one. We can
use <code>if let</code> syntax to concisely express this. The result of the
<code>compare_exchange</code> doesn't matter. If it succeeds, we performed the write. If it
doesn't someone else performed it. Also, notice how we are <code>compare_exchange</code>ing
an <code>AtomicU64</code>. The data is transmuted into those bytes, allowing us to make
atomic modifications to the contents of the vector. Because the data needs to be
transmuted into an atomic type, the vector can't support types larger than 8
bytes. Finally, because we are using <code>AcqRel</code> as the success ordering, any
subsequent <code>Acquire</code> loads will see the that there is no pending write.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    #[allow(unused_must_use)]
    if let Some(writedesc) = pending {
        AtomicU64::compare_exchange(
            writedesc.location,
            writedesc.old,
            writedesc.new,
            Ordering::AcqRel,
            Ordering::Relaxed,
        );

<span class="boring">}
</span></code></pre></pre>
<p>Now that we've done the store to the contents, we change the <code>WriteDescriptor</code>
status of the vector to indicate that there is no pending write (we just took
care of it!).</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        let new_writedesc = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();
        // # Safety
        // The pointer is valid to dereference because it started off valid
        // and only pointers made from WriteDescriptor::new_*_as_ptr()
        // (which are valid because of Box) are CAS'd in
        unsafe { &amp;*self.descriptor.load(Ordering::Acquire) } // Loading with `Acquire`
            .pending
            .store(new_writedesc, Ordering::Release); // Storing with `Release`

        // Memory leak alert!
        // What happens to the old pointer stored in the
        // `AtomicPtr&lt;Option&lt;WriteDescriptor&lt;T&gt;&gt;&gt;`?
        // We never reclaim it.
    }
}

<span class="boring">}
</span></code></pre></pre>
<p>This is standard. We make a new <code>WriteDescriptor</code> and store it with <code>Release</code> so
that all subsequent <code>Acquire</code> loads will see it.</p>
<h2 id="leaking-memory"><a class="header" href="#leaking-memory">Leaking memory</a></h2>
<p>Leaking memory is when you use memory but never free it. This is the first
chunck of code that leaks. Our <code>Descriptor</code> has a pointer to an
<code>Option&lt;WriteDescriptor&gt;</code>. When we store a different pointer, we lose the old
pointer forever. Since we never do anything to deallocate the memory pointed to
by the old pointer, like <code>Box::from_raw</code>, that memory will stay allocated until
the end of the program.</p>
<p>We can't just differectly free the memory though, as there could be another
thread reading it. Later on, I'm going to show you how we can use a data
structure called <em>hazard pointers</em> to safely reclaim objects.</p>
<p>For now, the vector will stay leaky, and we'll move on the <code>push</code>.</p>
<hr />
<h3 id="complete-source-for-complete_write"><a class="header" href="#complete-source-for-complete_write">Complete source for <code>complete_write()</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn complete_write(&amp;self, pending: &amp;Option&lt;WriteDescriptor&lt;T&gt;&gt;) {
    #[allow(unused_must_use)]
    if let Some(writedesc) = pending {
        AtomicU64::compare_exchange(
            writedesc.location,
            writedesc.old,
            writedesc.new,
            Ordering::AcqRel,
            Ordering::Relaxed,
        );
        let new_writedesc = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();
        // # Safety
        // The pointer is valid to dereference because it started off valid
        // and only pointers made from WriteDescriptor::new_*_as_ptr()
        // (which are valid because of Box) are CAS'd in
        unsafe { &amp;*self.descriptor.load(Ordering::Acquire) }
            .pending
            .store(new_writedesc, Ordering::Release);
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="push"><a class="header" href="#push"><code>push()</code></a></h1>
<p>You made it! We're going to implement half of the main functionality of the
vector. The code is going to get a little complex, but I'm confident in you. I
eventually understood what was going on, so you can too.</p>
<p>We're going to track the steps described in
<a href="code/ops/../../paper/algorithm.html"><strong>The Algorithm</strong></a> closely. We don't want to mess up
the concurrent semantics of the vector during implementation. The first thing we
do is load in the <code>Descriptor</code> and <code>WriteDescriptor</code>. This is actually harder
than it might seem, as we're working with <code>unsafe</code> things like raw pointers. We
need to be very careful. But wait, there's one more thing I want to cover, and
that's <em>exponential backoff</em>!</p>
<h2 id="exponential-backoff"><a class="header" href="#exponential-backoff">Exponential Backoff</a></h2>
<p>Exponential backoff is another one of those techniques that's unique to
concurrent programming. <code>compare_exchange</code> algorithms like the one we're
implementing can produce a lot of contention over a couple specific memory
locations. For example, may threads are trying to <code>compare_exchange</code> the
<code>AtomicPtr&lt;Descriptor&lt;T&gt;&gt;</code> stored in the vector. That spot in memory is
constantly bombarded with heavy atomic operations. One way we can alleviate this
is by waiting a little bit after failing to <code>compare_exchange</code>. The first time
we fail, we back off for <code>1</code> tick. If we fail again, we back off for <code>2</code> ticks,
then <code>4</code>, <code>8</code> . . . this is why the <em>backoff</em> is <em>exponential</em>. In some
mircobenchmarks I did, introducing exponential backoff greatly speeded up the
vector. <code>crossbeam_utils</code> has a useful little <code>struct</code> call <code>Backoff</code> that we're
going to use. Ok, back to the code.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn push(&amp;self, elem: T) {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        // # Safety
        // It is safe to dereference the raw pointers because they started off valid
        // and can only be CAS'd with pointers from `Box::into_raw`
        let current_desc = unsafe { &amp;*self.descriptor.load(Ordering::Acquire) };

        // Complete a pending write op if there is any
        let pending = unsafe { &amp;*current_desc.pending.load(Ordering::Acquire) };

<span class="boring">}
</span></code></pre></pre>
<p>There is already a lot going on here, in just these 10ish lines of code.
Firstly, we've instantiated a <code>Backoff</code>. A the bottom of the loop, if we failed
to <code>compare_exchange</code> in our new <code>Descriptor</code>, we'll call <code>Backoff::spin()</code> to
wait a little bit, then we'll come back up to the top and try again.</p>
<p>This code also contains a very <code>unsafe</code> operation: dereferencing a raw pointer.
The more I read about the dangers of raw pointers, the more scared I got.
Paraphrasing from
<a href="https://doc.rust-lang.org/book/ch19-01-unsafe-rust.html?highlight=raw%20pointer#dereferencing-a-raw-pointer">The Book</a>,
raw pointers aren't guaranteed to point to valid memory, aren't guaranteed to be
non-null, don't implement cleanup (like <code>Box</code>), and ignore all the aliasing
rules (<code>&amp;/&amp;mut</code> semantics).</p>
<p>After watching
<a href="https://www.youtube.com/watch?v=QAz-maaH0KM">Demystifying <code>unsafe</code> code</a> by Jon
Gjengset, I felt better. <code>unsafe</code> code isn't intrinsically bad, it is just code
that comes with an extra contract that we must uphold and document.</p>
<p>In the case of these first raw pointer dereferences, we know the dereference is
safe because the pointers to the <code>Descriptor</code> and <code>WriteDescriptor</code> come from
<code>Box::into_raw</code>, which returns a non-null and aligned pointer. <code>unsafe</code> is
scary, but not necessarily bad. Obviously, we should try to limit its uses as
much as possible though, as we can slip up and violate contracts.</p>
<blockquote>
<p>Mitigating <code>unsafe</code> code: there are ways we can construct API's that need
<code>unsafe</code> code to work without exposing users to danger. For example, we could
make a type <code>AtomicBox&lt;T&gt;</code> that's mostly a wrapper around <code>AtomicPtr&lt;T&gt;</code>. It
might look a little something like this:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[repr(transparent)]
struct AtomicBox&lt;T&gt; {
    ptr: AtomicPtr&lt;T&gt;
}

impl&lt;T&gt; AtomicBox&lt;T&gt; {
    // We can only make a `Self` from a `Box`'s pointer!
    pub fn new(box: Box&lt;T&gt;) -&gt; Self {
        AtomicPtr::new(Box::into_raw(box))
    }

    // Caller knows they are receiving a pointer from `Box`
    pub fn load(&amp;self, ordering: Ordering) -&gt; *mut T {
        self.0.load(ordering)
    }

    // -- snip --
}

<span class="boring">}
</span></code></pre></pre>
<p>There's nothing super crazy going on here, it's just that we've configured the
API so that we <strong>know</strong> the pointer inside the <code>AtomicBox&lt;T&gt;</code> is valid because
it could only have come from <code>Box</code>. Now, instead of manually ensuring the
invariant that we use <code>Box::into_raw</code> pointers, the compiler/type system does
so for us.</p>
</blockquote>
<p>After loading in the <code>WriteDescriptor</code>, execute it if need be:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    self.complete_write(pending);

<span class="boring">}
</span></code></pre></pre>
<p>Since we're <code>push</code>ing onto the vector, we might need more memory:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // Calculate which bucket this element is going into
    let bucket = (highest_bit(current_desc.size + FIRST_BUCKET_SIZE)
        - highest_bit(FIRST_BUCKET_SIZE)) as usize;

    // If the bucket is null, allocate the memory
    if self.buffers[bucket].load(Ordering::Acquire).is_null() {
        self.allocate_bucket(bucket)
    }

<span class="boring">}
</span></code></pre></pre>
<p>Let's make our new <code>WriteDescriptor</code> now:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    // # Safety
    // It is safe to call `self.get()` because if the vector has reached
    // `current_desc.size`, so there is a bucket allocated for element `size`.
    // Therefore, the pointer is also valid to dereference because it points
    // into properly allocated memory.
    let last_elem = unsafe { &amp;*self.get(current_desc.size) };
    let write_desc = WriteDescriptor::&lt;T&gt;::new_some_as_ptr(
        unsafe { mem::transmute_copy::&lt;T, u64&gt;(&amp;elem) },
        last_elem.load(Ordering::Acquire),
        last_elem,
    );

<span class="boring">}
</span></code></pre></pre>
<p>For now we are assuming that the vector is only storing values 8 bytes or
smaller, therefore it is safe to <code>transmute_copy</code> to an <code>AtomicU64</code>. I plan on
writing a macro that produces different implementations of the vector with
different atomic types when storing types of different sizes. For example,
<code>SecVec&lt;(i8, i8)&gt;</code> would store the data in <code>AtomicU16</code>.</p>
<p>Note that <code>last_elem</code>'s type is <code>&amp;AtomicU64</code>; it's the location of the write.
When we load from <code>last_elem</code>, we are getting the <code>old</code> element. We now have the
three pieces of data necessary for <code>compare_exchange</code>: a memory location (the
reference), an old element, and a new element (the <code>T</code> passed to this
function).// Load from the AtomicU64, which really contains the bytes for T</p>
<p>Let's put our package everything up in a <code>Descriptor</code> now:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(write_desc, current_desc.size + 1);

<span class="boring">}
</span></code></pre></pre>
<p>Since we are adding one more element onto the vector, the new <code>Descriptor</code>'s
size is one more than the old one's.</p>
<p>Here comes the crucial <code>compare_exchange</code>, in the <code>AcqRel/Relaxed</code> flavor:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    if AtomicPtr::compare_exchange_weak(
        &amp;self.descriptor,
        current_desc as *const _ as *mut _,
        next_desc,
        Ordering::AcqRel,
        Ordering::Relaxed,
    )
    .is_ok()
    {
        // We know the current write_desc is the one we just sent in
        // with the compare_exchange so avoid loading it atomically
        self.complete_write(unsafe { &amp;*write_desc });
        break;
    }

<span class="boring">}
</span></code></pre></pre>
<p>If the <code>compare_exchange</code> succeeds, we call <code>complete_write</code> on the descriptor
we just made to finalize the changes, then we <code>break</code> out of the loop.</p>
<p>If <code>compare_exchange</code> fails, we'll simply start over again.</p>
<p>Either way, <strong>we have a memory leak</strong>. If the <code>compare_exchange</code> succeeded, we
never deal with the old <code>Descriptor</code>'s pointer. We can never safely deallocate
it because we don't know if anyone is reading it. It would be terribly rude to
pull the rug out from under them! Also the deallocation would probably cause a
use-after-free which would cause the OS to terminate the program which would rip
a hole in the space-time continuum which would. Wait what? Uhh, moving on . . .</p>
<p>If the <code>compare_exchange</code> failed, the new <code>Descriptor</code> and <code>WriteDescriptor</code>
leak. Once we reach the end of the loop, all local variables in that scope are
lost. So, we never get back the pointers to our new describe-y objects, and
their memory is lost the the void, never to be seen again (unless we do some
wildly dumb stuff and read a random address or something). In any case, within
the code for the vector, I try not to tempt the segfault gods. My other
projects, maybe a little bit.</p>
<p>At this point, we've failed the <code>compare_exchange</code>. Let's <code>Backoff::spin()</code> and
then retry:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        backoff.spin();
    } // Closing brace for the loop
} // Closing brace for the function

<span class="boring">}
</span></code></pre></pre>
<p>Once we finish loop and finally succeed with the <code>compare_exchange</code>, we're done!
That's a <code>push</code>. The pseudocode is so simple, and the code is so . . . not
simple. Props to you for getting this far, concurrent programming is not for the
weak of spirit.</p>
<p>The rest of the book should be easier digest compared to <em>that</em>. I'll cover the
minor differences in <code>pop</code>, and then we'll cap off the code with <code>size</code>.</p>
<hr />
<h3 id="complete-source-for-push"><a class="header" href="#complete-source-for-push">Complete source for <code>push</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn push(&amp;self, elem: T) {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        // # Safety
        // It is safe to dereference the raw pointers because they started off valid
        // and can only be CAS'd with pointers from `Box::into_raw`
        let current_desc = unsafe { &amp;*self.descriptor.load(Ordering::Acquire) };
        let pending = unsafe { &amp;*current_desc.pending.load(Ordering::Acquire) };

        // Complete a pending write op if there is any
        self.complete_write(pending);

        // Allocate memory if need be
        let bucket = (highest_bit(current_desc.size + FIRST_BUCKET_SIZE)
            - highest_bit(FIRST_BUCKET_SIZE)) as usize;
        if self.buffers[bucket].load(Ordering::Acquire).is_null() {
            self.allocate_bucket(bucket)
        }
        // # Safety
        // It is safe to call `self.get()` because if the vector has reached
        // `current_desc.size`, so there is a bucket allocated for element `size`.
        // Therefore, the pointer is also valid to dereference because it points
        // into properly allocated memory.
        let last_elem = unsafe { &amp;*self.get(current_desc.size) };

        let write_desc = WriteDescriptor::&lt;T&gt;::new_some_as_ptr(
            unsafe { mem::transmute_copy::&lt;T, u64&gt;(&amp;elem) },
            last_elem.load(Ordering::Acquire),
            last_elem,
        );

        let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(write_desc, current_desc.size + 1);

        // Handle result of compare_exchange
        if AtomicPtr::compare_exchange_weak(
            &amp;self.descriptor,
            current_desc as *const _ as *mut _,
            next_desc,
            Ordering::AcqRel,
            Ordering::Relaxed,
        )
        .is_ok()
        {
            // We know the current write_desc is the one we just sent in
            // with the compare_exchange so avoid loading it atomically
            self.complete_write(unsafe { &amp;*write_desc });
            break;
        }

        backoff.spin();
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pop"><a class="header" href="#pop">pop</a></h1>
<p>There are three main differences between <code>pop</code> and <code>push</code>. Firstly, <code>pop</code> never
needs to allocate. Secondly, <code>pop</code> swaps in a slightly different descriptor,
with <code>None</code> as the <code>WriteDescriptor</code> and <code>current_desc.size - 1</code> as the new
size.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    let new_pending = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();
    let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(new_pending, current_desc.size - 1);

<span class="boring">}
</span></code></pre></pre>
<p>The final difference is that after we do the <code>compare_exchange</code>, we read the
last element and return it.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    if AtomicPtr::compare_exchange_weak(
        &amp;self.descriptor,
        current_desc as *const _ as *mut _,
        next_desc,
        Ordering::AcqRel,
        Ordering::Relaxed,
    )
    .is_ok()
    {
        // # Safety
        // This is ok because only 64-bit values can be stored in the vector
        // We also know that elem is a valid T because it was transmuted into a usize
        // from a valid T, therefore we are only transmuting it back
        return Some(unsafe { mem::transmute_copy::&lt;u64, T&gt;(&amp;elem) });
    }

<span class="boring">}
</span></code></pre></pre>
<p>The rest of the function: loading the <code>Descriptors</code>, <code>compare_exchange</code>,
<code>Backoff</code>, is identical.</p>
<p>Like <code>push</code>, <code>pop</code> also leaks memory profusely. Luckily, this means that when we implement memory reclamation, it'll be the same for <code>push</code> and <code>pop</code>.</p>
<hr />
<h3 id="complete-source-for-pop"><a class="header" href="#complete-source-for-pop">Complete source for <code>pop()</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn pop(&amp;self) -&gt; Option&lt;T&gt; {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        let current_desc = unsafe { &amp;*self.descriptor.load(Ordering::Acquire) };
        let pending = unsafe { &amp;*current_desc.pending.load(Ordering::Acquire) };

        self.complete_write(pending);
        if current_desc.size == 0 {
            return None;
        }

        // # Safety
        // Do not need to worry about underflow for the sub because we would have
        // already returned
        let elem = unsafe { &amp;*self.get(current_desc.size - 1) }
            .load(Ordering::Acquire);

        let write_desc = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();
        let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(write_desc, current_desc.size - 1);

        if AtomicPtr::compare_exchange_weak(
            &amp;self.descriptor,
            current_desc as *const _ as *mut _,
            next_desc,
            Ordering::AcqRel,
            Ordering::Relaxed,
        )
        .is_ok()
        {
            // # Safety
            // This is ok because only 64-bit values can be stored in the vector
            // We also know that elem is a valid T because it was transmuted into a
            // usize from a valid T, therefore we are only transmuting it back
            return Some(unsafe { mem::transmute_copy::&lt;u64, T&gt;(&amp;elem) });
        }
        backoff.spin();
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="size"><a class="header" href="#size"><code>size()</code></a></h1>
<p>The procedure for <code>size</code> is simple. Load the vector's size from the
<code>Descriptor</code>, then subtract one if there is a pending write.</p>
<p>It seems like so long ago that we went over
<a href="code/ops/../../paper/algorithm.html">The Algorithm</a>, but recall that when we perform a
<code>push</code>, we swap in a <code>Descriptor</code>, then call <code>complete_write</code>. This means that
when we do a write, the increase in size is reflected in the vector's state
before the write actually happens. If there is still a <code>WriteDescriptor</code>
contained in the <code>Descriptor</code>, that means the size stored in the <code>Descriptor</code> is
one greater than the actual size of the vector, because <code>complete_write</code>
replaces the <code>WriteDescriptor</code> with <code>None</code>.</p>
<p>Here is the code:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn size(&amp;self) -&gt; usize {
    // # Safety
    // The pointers are safe to dereference because they all came from `Box::into_raw`
    // and point to valid objects

    let desc = unsafe { &amp;*self.descriptor.load(Ordering::Acquire) };
    let size = desc.size;

    // If there is a pending descriptor, we subtract one from the size because
    // `push` increments the size, swaps the new descriptor in, and _then_ writes
    // the value. Therefore the size is one greater because the write hasn't happened
    // yet
    match unsafe { &amp;*desc.pending.load(Ordering::Acquire) } {
        Some(_) =&gt; size - 1,
        None =&gt; size,
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>There have been many momentous moments throughout the book: understanding the
algorithm, finishing <code>push</code>, and finally, completing the vector's public API.
When I was writing the code, this moment felt huge, and I jumped up and down after <code>push</code>ing 10 elements onto the vector, <code>pop</code>ing 10 times, and running <code>assert_eq!(sv.size(), 0);</code></p>
<p>Let's run some tests (more fun than you might think)!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="tests"><a class="header" href="#tests">Tests</a></h1>
<p>Just for fun, I wrote some tests, and we get to satisfyingly see them pass.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn size_starts_at_0() {
        let sv = SecVec::&lt;usize&gt;::new();
        assert_eq!(0, sv.size());
    }

    #[test]
    fn pop_empty_returns_none() {
        let sv = SecVec::&lt;usize&gt;::new();
        assert_eq!(sv.pop(), None);
    }

    #[test]
    fn ten_push_ten_pop() {
        let sv = SecVec::&lt;isize&gt;::new();
        for i in 0..10 {
            sv.push(i);
        }
        for i in (0..10).rev() {
            assert_eq!(sv.pop(), Some(i));
        }
    }

    #[test]
    fn does_not_allocate_buffers_on_new() {
        let sv = SecVec::&lt;isize&gt;::new();
        for buffer in &amp;**sv.buffers {
            assert!(buffer.load(Ordering::Relaxed).is_null())
        }
    }
}

<span class="boring">}
</span></code></pre></pre>
<p><code>Cargo</code> is super nice and we can use it to test. Running <code>cargo test</code> produces
the following output:</p>
<pre><code>~/C/r/unlocked (main) &gt; cargo test -- leaky::tests
    Finished test [unoptimized + debuginfo] target(s) in 0.01s
     Running unittests (target/debug/deps/unlocked-e6f64e7ba9c7e004)

running 4 tests
test leaky::tests::size_starts_at_0 ... ok
test leaky::tests::pop_empty_returns_none ... ok
test leaky::tests::does_not_allocate_buffers_on_new ... ok
test leaky::tests::ten_push_ten_pop ... ok
</code></pre>
<p>Although you can't see it, the green on those &quot;ok&quot;s warms my heart.</p>
<p>We know the vector is leaky, but otherwise it shouldn't be doing any other funky
things or UB. Let see if <code>Miri</code> finds anything with
<code>MIRIFLAGS=-Zmiri-ignore-leaks cargo miri test -- leaky::tests</code>:</p>
<pre><code>~/C/r/unlocked (main) &gt; MIRIFLAGS=-Zmiri-ignore-leaks cargo miri test -- leaky::tests
    Finished test [unoptimized + debuginfo] target(s) in 0.01s
     Running unittests (target/miri/x86_64-apple-darwin/debug/deps/unlocked-4269)

running 4 tests
test leaky::tests::does_not_allocate_buffers_on_new ... ok
test leaky::tests::pop_empty_returns_none ... ok
test leaky::tests::size_starts_at_0 ... ok
test leaky::tests::ten_push_ten_pop ... ok
</code></pre>
<p>Nothing? Awesome! Just because <code>Miri</code> doesn't find anything doesn't mean nothing
fishy is happening. <code>Miri</code> combined with the rigorous analysis of the code we
did though is a very good sign.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-reclamation"><a class="header" href="#memory-reclamation">Memory Reclamation</a></h1>
<p>Allocation isn't the hard part when it comes to concurrency, deallocation is.
When multiple threads/entities are concurrently accessing an object, it is
<strong>never</strong> safe to deallocate it without verifying that no one has a
reference/pointer to it. What is they were to use that pointer after the
deallocation?</p>
<p>This problem arises in the vector when deallocating the <code>Descriptor</code>s and
<code>WriteDescriptors</code>. Multiple threads can hold a reference to them at once, so we
never know when it is safe to deallocation.</p>
<p>To solve this problem, I used technique called <em>hazard pointers</em> via the
<a href="https://docs.rs/haphazard/latest/haphazard"><code>haphazard</code></a> crate.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hazard-pointers"><a class="header" href="#hazard-pointers">Hazard Pointers</a></h1>
<p>The idea of hazard pointers is to <em>protect</em> memory addresses from deallocation.
At any moment in time, we have a list of addresses that are not safe to reclaim.
We can store the addresses in a data structure like a concurrent linked lists; I
think this is what <code>haphazard</code>
<a href="https://docs.rs/haphazard/latest/src/haphazard/domain.rs.html#759-768">uses</a>.</p>
<p>Whenever we want to access a pointer, we access it through a <em>hazard pointer</em>.
When we access through a hazard pointer, the address we are accessing gets added
to the list of addresses to protect. When the hazard pointer get's dropped, or
we explicitly disassociate the hazard pointer from the underlying raw pointer,
the protection ends.</p>
<p>So why is this list important? When we are done with an object, we <em>retire</em> it.
By retiring the pointer, we are agreeing to not use it anymore. Any thread that
is already accessing it can continue to do so, but there can be not <em>new</em>
readers/writers.</p>
<p>Every once in a while, the <code>Domain</code>, which holds the hazard pointers will go
through the <code>Retired</code> list. For each pointer, the <code>Domain</code> checks whether that
pointer is protected by reading the <code>Protected</code> list. If the pointer isn't
protected, the <code>Domain</code> deallocates it. If it is protected, the <code>Domain</code> does
not reclaim it, because someone is using it. In this way, we prevent pointers in
use from being deallocated, but those out of use are deallocated.</p>
<h2 id="an-example"><a class="header" href="#an-example">An example</a></h2>
<p>Hazard pointers are pretty complicated, so here's a visual example that I hope
helps.</p>
<pre><code>Protected: [1&lt;0x22&gt;]
Retired: []
              0x20   0x22   0x23   0x24
            +------+------+------+------+
Thread 1    |      |  &lt;&gt;  |      |      |
Thread 2    |      |      |      |      |
            +------+------+------+------+
</code></pre>
<p>Right now Thread 1 is accessing <code>0x22</code> via a hazard pointer, so the <code>Protected</code>
list contains the pointer <code>Ox22</code>, annotated with <code>1</code> to indicate Thread 1 is
protecting it. I'm not sure if you would actually keep track of which thread is
protecting a pointer in an actual implementation. I think if another thread
tries to protect a pointer, if it's already protected, nothing will happen.</p>
<p>Ok, now, Thread 2 accesses <code>0x22</code> and protects the pointer.</p>
<pre><code>Protected: [1&lt;0x22&gt;, 2&lt;0x22&gt;]
Retired: []
              0x20   0x22   0x23   0x24
            +------+------+------+------+
Thread 1    |      |  &lt;&gt;  |      |      |
Thread 2    |      |  &lt;&gt;  |      |      |
            +------+------+------+------+
</code></pre>
<p>Thread 1 finishes with its access, and retires <code>0x22</code>. Thread 1 is saying, &quot;No
one new will use this pointer, deallocate it when it's safe to do so!&quot; <code>0x22</code> is
added to the <code>Retired</code> list. The <code>Domain</code> can't retire the pointer yet because
Thread 2 is still accessing it.</p>
<pre><code>Protected: [2&lt;0x22&gt;]
Retired: [0x22]
              0x20   0x22   0x23   0x24
            +------+------+------+------+
Thread 1    |      |      |      |      |
Thread 2    |      |  &lt;&gt;  |      |      |
            +------+------+------+------+
</code></pre>
<p>Finally, Thread 2 finishes using the pointer, removing <code>0x22</code> from the
<code>Protected</code> list.</p>
<pre><code>Protected: []
Retired: [0x22]
              0x20   0x22   0x23   0x24
            +------+------+------+------+
Thread 1    |      |      |      |      |
Thread 2    |      |      |      |      |
            +------+------+------+------+
</code></pre>
<p>The <code>Domain</code> sees that <code>0x22</code> is retired and no one is protecting it, so it
deallocates the allocation at <code>0x22</code>. We have reclaimed memory, and <code>0x22</code> will
not leak!</p>
<h2 id="code-changes"><a class="header" href="#code-changes">Code changes</a></h2>
<p>To use the hazard pointers, we're going to need to make a small change in the
vector's structure.</p>
<p>The hardest part was getting started.</p>
<p>Following the documentation on
<a href="https://docs.rs/haphazard/latest/haphazard/struct.Domain.html"><code>Domain</code></a>, I
wrote a bunch of type <code>alias</code>es using the <code>type</code> keyword:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Setting up hazard pointers
// This makes sure they all use the same Domain, guaranteeing the protection is valid.
#[non_exhaustive]
struct Family;
type Domain = haphazard::Domain&lt;Family&gt;;
type HazardPointer&lt;'domain&gt; = haphazard::HazardPointer&lt;'domain, Family&gt;;
type HazAtomicPtr&lt;T&gt; = haphazard::AtomicPtr&lt;T, Family&gt;;
<span class="boring">}
</span></code></pre></pre>
<p>This makes sure that we only uses <code>Domain</code>s produced from struct <code>Family</code>. This
prevents us from retiring a pointer in the <code>Global</code> domain that is being guarded
in a different domain. The <code>Global</code> domain can't see the other <code>Domain</code>'s
protected list, so might prematurely retire the pointer.</p>
<p>Secondly, all the <code>HazardPointer</code>s and <code>HazAtomicPtr</code>s we construct will be in
same family as our <code>Domain</code>s. This ensures the same protection against
overlapping with the <code>Global</code> domain.</p>
<blockquote>
<p>The difference between <code>HazAtomicPtr</code> which is an an alias for
<code>haphazard::AtomicPtr</code>, and <code>std::sync::atomic::AtomicPtr</code>, is that
<code>HazAtomicPtr</code> uses hazard pointers to guard loads. Additionally, all atomic
operations with <code>HazAtomicPtr</code> have <code>Acquire-Release</code> semantics built in.
Nifty!</p>
</blockquote>
<p>To ensure that we always retire and protect in the same domain, we will also
carry a <code>Domain</code> in the <code>struct</code> itself. Then, it's pretty easy to just always
use <code>&amp;self.domain</code> whenever we need a domain. All we have to do is add one more
<code>struct</code> field to <code>SecVec</code>:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub struct SecVec&lt;'a, T: Sized + Copy&gt; {
    buffers: CachePadded&lt;Box&lt;[AtomicPtr&lt;AtomicU64&gt;; 60]&gt;&gt;,
    descriptor: CachePadded&lt;HazAtomicPtr&lt;Descriptor&lt;'a, T&gt;&gt;&gt;,
    domain: Domain, // Hi there :)
    _boo: PhantomData&lt;T&gt;,
}

struct Descriptor&lt;'a, T: Sized&gt; {
    pending: HazAtomicPtr&lt;Option&lt;WriteDescriptor&lt;'a, T&gt;&gt;&gt;,
    size: usize,
}

struct WriteDescriptor&lt;'a, T: Sized&gt; {
    new: u64,
    old: u64,
    location: &amp;'a AtomicU64,
    _boo: PhantomData&lt;T&gt;,
}
<span class="boring">}
</span></code></pre></pre>
<p>And with that out of the way, we can now plug some leaks!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fixing-complete_write"><a class="header" href="#fixing-complete_write">Fixing complete_write</a></h1>
<p><code>complete_write</code> was the easiest leak to seal. When we swap out the
<code>WriteDescriptor</code>, we get back the old one. All we have to do is retire it, and
its memory will eventually get reclaimed. Using the <code>haphazard</code> crate did
require some slight changes to code, which I'll point out.</p>
<p>We execute the <code>WriteDescriptor</code> and make a new one (<code>None</code>) like normal:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn complete_write(&amp;self, pending: *mut Option&lt;WriteDescriptor&lt;T&gt;&gt;) {
    // If cas of actual value fails, someone else did the write
    // Result of compare_exchange doesn't matter
    if let Some(writedesc) = unsafe { &amp;*pending } {
        let _ = AtomicU64::compare_exchange(
            writedesc.location,
            writedesc.old,
            writedesc.new,
            Ordering::AcqRel,
            Ordering::Relaxed,
        );

        let new_writedesc = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();

<span class="boring">}
</span></code></pre></pre>
<p>Here comes the part where the hazard pointers kick in. We make a hazard pointer
in <code>&amp;self.domain</code>, then load in the <code>Descriptor</code>. Now, the current <code>Descriptor</code>
cannot get deallocated as long as our hazard pointer is alive. Then we swap in a
new pointer to the <code>None</code> <code>WriteDescriptor</code>.</p>
<p>Here comes the big change, instead of just doing nothing with the pointer, we
<code>retire</code> it in <code>&amp;self.domain</code>. According to the documentation for <code>retire_in</code>,
there is a safety contract we need to follow (hence the marking <code>unsafe fn</code>).</p>
<p>Let's look at that:</p>
<blockquote>
<ol>
<li>The pointed-to object will never again be returned by any
<code>[Haz]AtomicPtr::load</code>.</li>
<li>The pointed-to object has not already been retired.</li>
<li>All calls to load that can have seen the pointed-to object were using
hazard pointers from domain.</li>
</ol>
</blockquote>
<p>Alright, let's make sure we're fulfilling the contract.</p>
<p>Number one, we swapped out the pointer, so all new calls to <code>HazAtomicPtr::load</code>
will use the new pointer. This is <code>Acquire-Release</code> semantics in action under
the hood. Since the <code>swap_ptr</code> uses <code>Release</code>, all <code>HazAtomicPtr::load</code>s (which
use <code>Acquire</code>) will see the new value. Thus, the old value is safe from being
<code>load</code>ed again.</p>
<p>Number two, only one thread can get a pointer as the result of a swap. If I'm
holding a marble in my hand and I give it away, no one else can take that marble
from my hand. The person who took it can do whatever they want with it without
worrying about others interfering. Since we got the pointer as the result of
<code>swap_ptr</code>, no other thread has exclusive access like we do. We took the marble.
Therefore, we know that know other thread has already or might retire the
pointer. They can't access the marble anymore, and if we have the marble, it
means they never had it.</p>
<p>Finally, number 3, all operations (creating hazard pointers, retiring pointers)
happen through <code>&amp;self.domain</code>!</p>
<p>After writing a 1000 word essay, we can confirm that <code>retire_in</code> is safe to
call. This is the argument we'll use for <code>retire</code>ing the results of <code>compare_exchange</code> in
<code>push</code>/<code>pop</code>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        let mut hp = HazardPointer::new_in_domain(&amp;self.domain);

        let old = unsafe {
            self.descriptor
                .load(&amp;mut hp)
                .expect(&quot;ptr is null&quot;)
                .pending // This is a HazAtomicPtr&lt;WriteDescriptor&gt;
                // # Safety
                // new_writedesc conforms to the requirements of HazAtomicPtr::new()
                // because it comes from Box::into_raw and is a valid WriteDescriptor
                .swap_ptr(new_writedesc)
        };

        // # Safety
        // We are the only thread that will retire this pointer because
        // only one thread can get the result of the swap (this one).
        // Two threads couldn't have performed a swap and both got this pointer.
        unsafe { old.unwrap().retire_in(&amp;self.domain) };

        // hp gets dropped, protection ends
    }
}

<span class="boring">}
</span></code></pre></pre>
<p>That's the only change to <code>complete_write</code>. <code>push</code>/<code>pop</code> aren't much worse.</p>
<hr />
<h3 id="complete-source-for-complete_write-not-leaky"><a class="header" href="#complete-source-for-complete_write-not-leaky">Complete source for <code>complete_write</code> (not leaky)</a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn complete_write(&amp;self, pending: *mut Option&lt;WriteDescriptor&lt;T&gt;&gt;) {
    // If cas of actual value fails, someone else did the write
    // Result of cmpxchng doesn matter
    if let Some(writedesc) = unsafe { &amp;*pending } {
        let _ = AtomicU64::compare_exchange(
            writedesc.location,
            writedesc.old,
            writedesc.new,
            Ordering::AcqRel,
            Ordering::Relaxed,
        );

        let new_writedesc = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();

        let mut hp = HazardPointer::new_in_domain(&amp;self.domain);

        let old = unsafe {
            self.descriptor
                .load(&amp;mut hp)
                .unwrap()
                .pending
                // # Safety
                // new_writedesc conforms to the requirements of HazAtomicPtr::new()
                // because it comes from Box::into_raw and is a valid WriteDescriptor
                .swap_ptr(new_writedesc)
        };

        // # Safety
        // We are the only thread that will retire this pointer because
        // only one thread can get the result of the swap (this one).
        // Two threads couldn't have performed a swap and both got this pointer.
        unsafe { old.unwrap().retire_in(&amp;self.domain) };
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="fixing-push--pop"><a class="header" href="#fixing-push--pop">Fixing push &amp; pop</a></h1>
<p>I know I said that the changes to <code>push</code> and <code>pop</code> aren't that bad, which is
true. Getting to those changes however, took a while. I'm going to explain what
I did with pseudocode first, and then show the final code.</p>
<p>The first think I tried was just retiring the old <code>Descriptor</code> after a
successful <code>compare_exchange</code>, however, this didn't reduce the leakage at all
for some reason. I figured it might be because the <code>Descriptor</code> was pointing a
live <code>WriteDescriptor</code>. So then, I also retired the <code>WriteDescriptor</code>. However,
this produced use-after-frees and data races according to <code>Miri</code>, so I knew I
was doing something wrong.</p>
<p>I decided to review the safety contract of <code>retire_in</code> again, and that is when I
found the bug. Retiring the <code>Descriptor</code> is safe for the same reason retiring
the <code>WriteDescriptor</code> after <code>complete_write</code> is. Since the <code>Descriptor</code> is the
result of a swap, we are the only thread who will retire it. The thing is, if we
also retire the <code>WriteDescriptor</code>, a thread who is already accessing the
<code>Descriptor</code> could make a <em>new</em> load to the just retired <code>WriteDescriptor</code>,
violating the safety contract of <code>reitre_in</code>, and causing UB.</p>
<h2 id="the-problem-in-picture-form"><a class="header" href="#the-problem-in-picture-form">The problem in picture form</a></h2>
<p>We, Thread 1, have the <code>Descriptor</code> as the result of a successful
<code>compare_exchange</code>. Thread 2 is also reading the <code>Descriptor</code> (<em>but not the
inner <code>WriteDescriptor</code></em>)</p>
<pre><code>               Thread 2
               /
Thread 1 (us) /
   |         /
   |        /
   V       v
  Descriptor
     \
      \
       \
        v
        WriteDescriptor
</code></pre>
<p>So, we <code>retire</code> the <code>Descriptor</code> and <code>WriteDescriptor</code>. The <code>Descriptor</code> is
protected from reclamation because Thread 2 is reading it, but the
<code>WriteDescriptor</code> has no readers so it gets deallocated.</p>
<pre><code>               Thread 2
               /
Thread 1 (us) /
   |         /
   |        /
   V       v
  Descriptor
     \
   ---+----------------
       \
        v
        WriteDescriptor &lt;Deallocated&gt;
</code></pre>
<p>Now, Thread 2 goes to read the (now deallocated!!) <code>WriteDescriptor</code> by loading
the pointer through the <code>Descriptor</code> (which is still protected, and safe to
access).</p>
<pre><code>               Thread 2
                  |
Thread 1 (us)     |
   |              |
   |              |
   V              |
  Descriptor      |
     \            |
   ---+-----------+----
       \          |
        v         V
        WriteDescriptor &lt;Deallocated&gt;
</code></pre>
<p>And here we have it, Thread 2 accessing deallocated memory!</p>
<h2 id="the-solution"><a class="header" href="#the-solution">The solution</a></h2>
<p>The solution I came up with is to make sure a reference to a <code>WriteDescriptor</code>
never outlives the reference to it's parent <code>Descriptor</code>. Visually this looks
like:</p>
<pre><code>-- Descriptor Reference Start

    -- WriteDescriptor Reference Start


    -- WriteDescriptor Reference End



-- Descriptor Reference End
</code></pre>
<p>This means that when there are no people accessing a <code>Descriptor</code>, there are
also no people accessing the innter <code>WriteDescriptor</code>. Therefore, when a
<code>Descriptor</code> is <code>retired</code>ed, the <code>WriteDescriptor</code> is also safe to <code>retire</code>
because there are no references to it. Since no one can get a new reference to a
<code>retire</code>ed <code>Descriptor</code>, no once can access the inner <code>WriteDescriptor</code>.</p>
<p>Why is this important? Whenever we reclaim a <code>Descriptor</code>, we also reclaim the
inner <code>WriteDescriptor</code>, fixing our leaks without causing any UB.</p>
<p>To implement this custom behaviour for <code>Descriptor</code>, we implement the <code>Drop</code>
trait. A trait that implements <code>Drop</code> executes some custom behaviour when it
goes out of scope and is reclaimed.</p>
<p>The <code>Drop</code> implemenation looks like this:</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;T&gt; Drop for Descriptor&lt;'_, T&gt;
{
    fn drop(&amp;mut self) {
        // # Safety
        // The pointer is valid because it's from Box::into_raw
        // We must also ensure ref to wdesc never outlasts ref to desc
        unsafe {
            Box::from_raw(
                self.pending
                    .swap_ptr(ptr::null_mut())
                    .unwrap()
                    .into_inner() // This is a NonNull&lt;T&gt;
                    .as_ptr() // Turn it into a raw pointer
            );
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>All we're doing is extracting the pointer to the <code>WriteDescriptor</code> and calling
<code>Box::from_raw()</code> on it so that. When it goes out of scope (at the end of the
function), its memory will be reclaimed by <code>Box</code>.</p>
<h2 id="reclaiming-the-descriptors"><a class="header" href="#reclaiming-the-descriptors">Reclaiming the <code>Descriptor</code>s</a></h2>
<p>Its time to finally go over the code changes to <code>push</code>. All access to the
<code>Descriptor</code> and <code>WriteDescriptor</code> are guarded with a hazard pointer. The access
returns a reference to the <code>Descriptor</code>/<code>WriteDescriptor</code>, which is valid as
long as the hazard pointer guarding the access is alive. Access to the inner
<code>WriteDescriptor</code> is explicitly scoped within its own block to make clear that
access to the <code>WriteDescriptor</code> cannot oulive access to the parent <code>Descriptor</code>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn push(&amp;self, elem: T) {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        let mut dhp = HazardPointer::new_in_domain(&amp;self.domain);
        let current_desc = unsafe { self.descriptor.load(&amp;mut dhp) }
            .expect(&quot;invalid ptr for descriptor in push&quot;);

        // Use a block to make explicit that the use of the wdesc does not outlive
        // the use of the desc.
        // This means that when the desc is dropped, there will be no references
        // to the wdesc inside.
        // And we can deallocate the wdesc with `Box::from_raw`
        {
            let mut wdhp = HazardPointer::new_in_domain(&amp;self.domain);
            let pending = unsafe { current_desc.pending.load(&amp;mut wdhp) }
                .expect(&quot;invalid ptr from write-desc in push&quot;);

            self.complete_write(pending as *const _ as *mut _);
            // Hazard pointer is dropped, protection ends
        }
<span class="boring">}
</span></code></pre></pre>
<p>This stuff is all the same as before.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        // If we need more memory, calculate the bucket
        let bucket = (highest_bit(current_desc.size + FIRST_BUCKET_SIZE)
            - highest_bit(FIRST_BUCKET_SIZE)) as usize;
        // Allocate it
        if self.buffers[bucket].load(Ordering::Acquire).is_null() {
            self.allocate_bucket(bucket)
        }

        let last_elem = unsafe { &amp;*self.get(current_desc.size) };

        let next_write_desc = WriteDescriptor::&lt;T&gt;::new_some_as_ptr(
            // TODO: address this in macro
            // # Safety
            // The `transmute_copy` is safe because we have ensured that T is the
            // correct size at compile time
            unsafe { mem::transmute_copy::&lt;T, u64&gt;(&amp;elem) },
            // Load from the AtomicU64, which really containes the bytes for T
            last_elem.load(Ordering::Acquire),
            last_elem,
        );

        let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(next_write_desc,
            current_desc.size + 1);

<span class="boring">}
</span></code></pre></pre>
<p>The <code>compare_exchange</code> syntax is slightly different, but it's doing the exact
same thing. We don't have to specify <code>Ordering</code>s because it's built in by the
<code>haphazard</code> crate. On a successful <code>compare_exchange</code>, we <code>retire</code> the pointer
to the old <code>WriteDescriptor</code>. When it is finally reclaimed, its <code>Drop</code>
implementation will run and its inner <code>WriteDescriptor</code> will also get reclaimed
safely.</p>
<p>If the <code>compare_exchange</code> fails, we deallocate our local <code>Descriptor</code> normally
by calling <code>Box::from_raw</code>. Since the local <code>Descriptor</code> was never shared across
threads, we don't have to worry about synchronizing the deallocation. Then, we
spin using the <code>Backoff</code> and go back to the top of the loop.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>        if let Ok(replaced) = unsafe {
            HazAtomicPtr::compare_exchange_weak_ptr(
                // # Safety
                // Safe because the pointer we swap in points to a valid object that
                // is !null
                &amp;self.descriptor,
                current_desc as *const _ as *mut _,
                next_desc,
            )
        } {
            self.complete_write(next_write_desc);

            // # Safety
            // Since the we only retire when swapping out a pointer, this is the only
            // thread that will retire, since only one thread receives the result of
            // the swap (this one)
            //
            // There will never be another load call to the ptr because all calls will
            // go the new one. Since all uses of the inner wdesc are contained within
            // the lifetime of the reference to the desc, there will also be no new
            // loads on the inner wdesc.
            unsafe {
                replaced.unwrap().retire_in(&amp;self.domain);
            }
            break;
        }

        // Deallocate the write_desc and desc that we failed to swap in
        // # Safety
        // Box the write_desc and desc ptrs were made from Box::into_raw, so it
        // is safe to Box::from_raw
        unsafe {
            // Note: the inner wdesc also get's dropped as part of the desc's drop impl
            Box::from_raw(next_desc);
        }

        backoff.spin();
    }
}

<span class="boring">}
</span></code></pre></pre>
<p>The changes for <code>pop</code> are identical. We are so close to being done with code.
Our <code>Descriptor</code>s and <code>WriteDescriptors</code> are eventually reclaimed, which is a
big step forward. The last thing is to deallocate the buckets and the final
<code>Descriptor</code> when the vector itself is dropped.</p>
<hr />
<h3 id="complete-source-for-push-and-pop"><a class="header" href="#complete-source-for-push-and-pop">Complete source for <code>push()</code> and <code>pop()</code></a></h3>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn push(&amp;self, elem: T) {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        let mut dhp = HazardPointer::new_in_domain(&amp;self.domain);
        let current_desc = unsafe { self.descriptor.load(&amp;mut dhp) }
            .expect(&quot;invalid ptr for descriptor in push&quot;);

        // Use a block to make explicit that the use of the wdesc does not
        // outlive the use of the desc. This means that when the desc is dropped,
        // there will be no references to the wdesc inside.And we can deallocate
        // the wdesc with `Box::from_raw`
        {
            let mut wdhp = HazardPointer::new_in_domain(&amp;self.domain);
            let pending = unsafe { current_desc.pending.load(&amp;mut wdhp) }
                .expect(&quot;invalid ptr from write-desc in push&quot;);

            self.complete_write(pending as *const _ as *mut _);
            // Hazard pointer is dropped, protection ends
        }

        // If we need more memory, calculate the bucket
        let bucket = (highest_bit(current_desc.size + FIRST_BUCKET_SIZE)
            - highest_bit(FIRST_BUCKET_SIZE)) as usize;
        // Allocate it
        if self.buffers[bucket].load(Ordering::Acquire).is_null() {
            self.allocate_bucket(bucket)
        }

        let last_elem = unsafe { &amp;*self.get(current_desc.size) };

        let next_write_desc = WriteDescriptor::&lt;T&gt;::new_some_as_ptr(
            // TODO: address this in macro
            // # Safety
            // The `transmute_copy` is safe because we have ensured that T is
            // the correct size at compile time
            unsafe { mem::transmute_copy::&lt;T, u64&gt;(&amp;elem) },
            // Load from the AtomicU64, which really containes the bytes for T
            last_elem.load(Ordering::Acquire),
            last_elem,
        );

        let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(next_write_desc,
            current_desc.size + 1);

        if let Ok(replaced) = unsafe {
            HazAtomicPtr::compare_exchange_weak_ptr(
                // # Safety
                // Safe because the pointer we swap in points to a valid object that
                // is !null
                &amp;self.descriptor,
                current_desc as *const _ as *mut _,
                next_desc,
            )
        } {
            self.complete_write(next_write_desc);

            // # Safety
            // Since the we only retire when swapping out a pointer, this is the only
            // thread that will retire, since only one thread receives the result of
            // the swap (this one)
            //
            // There will never be another load call to the ptr because all calls will
            // go the new one. Since all uses of the inner wdesc are contained within
            // the lifetime of the reference to the desc, there will also be no new
            // loads on the inner wdesc.
            unsafe {
                replaced.unwrap().retire_in(&amp;self.domain);
            }
            break;
        }

        // Deallocate the write_desc and desc that we failed to swap in
        // # Safety
        // Box the write_desc and desc ptrs were made from Box::into_raw, so it is
        // safe to Box::from_raw
        unsafe {
            // Note: the inner wdesc also get's dropped as part of the desc's drop impl
            Box::from_raw(next_desc);
        }

        backoff.spin();
    }
}

pub fn pop(&amp;self) -&gt; Option&lt;T&gt; {
    let backoff = Backoff::new(); // Backoff causes significant speedup
    loop {
        let mut dhp = HazardPointer::new_in_domain(&amp;self.domain);
        let current_desc = unsafe { self.descriptor.load(&amp;mut dhp) }
            .expect(&quot;invalid ptr for descriptor in pop&quot;);

        // Use a block to make explicit that the use of the wdesc does not
        // outlive the use of the desc. This means that when the desc is
        //  dropped, there will be no references to the wdesc inside.
        // And we can deallocate the wdesc with `Box::from_raw`
        {
            let mut wdhp = HazardPointer::new_in_domain(&amp;self.domain);
            let pending = unsafe { current_desc.pending.load(&amp;mut wdhp) }
                .expect(&quot;invalid ptr for write-descriptor in pop&quot;);

            self.complete_write(pending as *const _ as *mut _);
            // Hazard pointer is dropped, protection ends
        }

        if current_desc.size == 0 {
            return None;
        }

        // TODO: add safety comment
        // Consider if new desc is swapped in, can we read dealloced memory?
        // Do not need to worry about underflow for the sub because we would
        // have already returned
        let elem = unsafe { &amp;*self.get(current_desc.size - 1) }
            .load(Ordering::Acquire);

        let new_pending = WriteDescriptor::&lt;T&gt;::new_none_as_ptr();

        let next_desc = Descriptor::&lt;T&gt;::new_as_ptr(new_pending,
            current_desc.size - 1);

        if let Ok(replaced) = unsafe {
            HazAtomicPtr::compare_exchange_weak_ptr(
                // # Safety
                // Safe because the pointer we swap in points to a valid object that
                // is !null
                &amp;self.descriptor,
                current_desc as *const _ as *mut _,
                next_desc,
            )
        } {
            // # Safety
            // Since the we only retire when swapping out a pointer, this is the only
            // thread that will retire, since only one thread receives the result of
            // the swap (this one)
            //
            // There will never be another load call to the ptr because all calls will
            // go the new one. Since all uses of the inner wdesc are contained within
            // the lifetime of the reference to the desc, there will also be no new
            // loads  on the inner wdesc.
            unsafe {
                replaced.unwrap().retire_in(&amp;self.domain);
            }

            // # Safety
            // TODO: address this in macro
            // This is ok because we ensure T is the correct size at compile time
            // We also know that elem is a valid T because it was transmuted into a
            // usize from a valid T, therefore we are only transmuting it back
            return Some(unsafe { mem::transmute_copy::&lt;u64, T&gt;(&amp;elem) });
        }

        // Deallocate the write_desc and desc that we failed to swap in
        // # Safety
        // Box the write_desc and desc ptrs were made from Box::into_raw, so
        // it is safe to Box::from_raw
        unsafe {
            // Note: the inner wdesc also get's dropped as part of the desc's drop impl
            Box::from_raw(next_desc);
        }

        backoff.spin();
    }
}
<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dropping-the-vector"><a class="header" href="#dropping-the-vector">Dropping the vector</a></h1>
<p>We approach the end! As its last action, the vector will free the mmeory
allocated in its buckets and the <code>Descriptor</code> it holds. Once again, we achieve
this by implementing the <code>Drop</code> trait.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;T&gt; Drop for SecVec&lt;'_, T&gt;
where
    T: Copy,
{
    fn drop(&amp;mut self) {
        // Drop buffers
        let allocator = Global;
        for (bucket, ptr) in self
            .buffers
            .iter()
            .filter(|ptr| !ptr.load(Ordering::Relaxed).is_null())
            .enumerate()
        // Getting all non-null buckets
        {
            let size = FIRST_BUCKET_SIZE * (1 &lt;&lt; bucket);
            let layout = match Layout::array::&lt;AtomicU64&gt;(size) {
                Ok(layout) =&gt; layout,
                Err(_) =&gt; capacity_overflow(),
            };
            unsafe {
                // # Safety
                // We have recreated the exact same layout used to alloc the ptr in `allocate_bucket`
                // We know the ptr isn't null becase of the filer
                allocator.deallocate(
                    NonNull::new(ptr.load(Ordering::Relaxed) as *mut u8).unwrap(),
                    layout,
                )
            };
        }

        // Retiring the current desc and wdesc
        // # Safety
        // Since we have &amp;mut self, we have exclusive access, so we can retire the desc and wdesc ptrs.
        // It is safe to deref the ptr to the desc because it is valid because it was created with
        // Descriptor::new_as_ptr.
        let desc = self.descriptor.load_ptr();
        unsafe {
            Box::from_raw(desc);
        };
    }
}

<span class="boring">}
</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="more-tests"><a class="header" href="#more-tests">More tests</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reflections"><a class="header" href="#reflections">Reflections</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="acknowledgements"><a class="header" href="#acknowledgements">Acknowledgements</a></h1>
<ul>
<li>Mr. McG</li>
</ul>
<h2 id="inspired-by"><a class="header" href="#inspired-by">Inspired by</a></h2>
<ul>
<li>Jon Gjenset</li>
<li>fasterthanlime</li>
</ul>
<h2 id="sources"><a class="header" href="#sources">Sources</a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="helpful-resources"><a class="header" href="#helpful-resources">Helpful Resources</a></h1>
<ul>
<li><a href="https://doc.rust-lang.org/stable/std/">Rust Standard Library Documentation</a></li>
<li><a href="https://www.youtube.com/c/JonGjengset">Jon Gjenset's Youtube Channel</a></li>
<li><a href="https://doc.rust-lang.org/stable/book/">The Book</a></li>
<li><a href="https://doc.rust-lang.org/stable/reference/">The Rust Reference</a></li>
<li><a href="https://doc.rust-lang.org/nightly/nomicon/intro.html">The Rustonomicon</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script type="text/javascript">
            var socket = new WebSocket("ws://localhost:3000/__livereload");
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script type="text/javascript">
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>

        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->

        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </body>
</html>
